[
    {
        "year": "2021",
        "name": "23rd ICMI 2021",
        "info": "Montr\u00e9al, QC, Canada",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2021",
                "sub_name": "ICMI '21: International Conference on Multimodal Interaction, Montr\u00e9al, QC, Canada, October 18-22, 2021.",
                "count": 127,
                "papers": [
                    "Incorporating Haptics into the Theatre of Multimodal Experience design: and the Ecosystem this Requires.",
                    "Theory Driven Approaches to the Design of Multimodal Assessments of Learning, Emotion, and Self-Regulation in Medicine.",
                    "Socially Interactive Artificial Intelligence: Past, Present and Future.",
                    "From Differentiable Reasoning to Self-supervised Embodied Active Learning.",
                    "Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis.",
                    "Exploiting the Interplay between Social and Task Dimensions of Cohesion to Predict its Dynamics Leveraging Social Sciences.",
                    "Dynamic Mode Decomposition with Control as a Model of Multimodal Behavioral Coordination.",
                    "A Contrastive Learning Approach for Compositional Zero-Shot Learning.",
                    "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning.",
                    "A Multimodal Dataset and Evaluation for Feature Estimators of Temporal Phases of Anxiety.",
                    "Inclusive Action Game Presenting Real-time Multimodal Presentations for Sighted and Blind Persons.",
                    "ViCA: Combining visual, Social, and Task-orientedconversational AI in a Healthcare Setting.",
                    "Towards Sound Accessibility in Virtual Reality.",
                    "Am I Allergic to This? Assisting Sight Impaired People in the Kitchen.",
                    "MindfulNest: Strengthening Emotion Regulation with Tangible User Interfaces.",
                    "A Systematic Cross-Corpus Analysis of Human Reactions to Robot Conversational Failures.",
                    "Recognizing Perceived Interdependence in Face-to-Face Negotiations through Multimodal Analysis of Nonverbal Behavior.",
                    "Modelling and Predicting Trust for Developing Proactive Dialogue Strategies in Mixed-Initiative Interaction.",
                    "Recognizing Social Signals with Weakly Supervised Multitask Learning for Multimodal Dialogue Systems.",
                    "Decision-Theoretic Question Generation for Situated Reference Resolution: An Empirical Study and Computational Model.",
                    "Digital Speech Makeup: Voice Conversion Based Altered Auditory Feedback for Transforming Self-Representation.",
                    "Hierarchical Classification and Transfer Learning to Recognize Head Gestures and Facial Expressions Using Earbuds.",
                    "Integrated Speech and Gesture Synthesis.",
                    "Co-Verbal Touch: Enriching Video Telecommunications with Remote Touch Technology.",
                    "HapticLock: Eyes-Free Authentication for Mobile Devices.",
                    "The Impact of Prior Knowledge on the Effectiveness of Haptic and Visual Modalities for Teaching Forces.",
                    "Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents.",
                    "Attachment Recognition in School Age Children Based on Automatic Analysis of Facial Expressions and Nonverbal Vocal Behaviour.",
                    "Characterizing Children's Motion Qualities: Implications for the Design of Motion Applications for Children.",
                    "Temporal Graph Convolutional Network for Multimodal Sentiment Analysis.",
                    "Conversational Group Detection with Graph Neural Networks.",
                    "Self-supervised Contrastive Learning of Multi-view Facial Expressions.",
                    "What's Fair is Fair: Detecting and Mitigating Encoded Bias in Multimodal Models of Museum Visitor Attention.",
                    "Bias and Fairness in Multimodal Machine Learning: A Case Study of Automated Video Interviews.",
                    "Technology as Infrastructure for Dehumanization: : Three Hundred Million People with the Same Face.",
                    "Investigating Trust in Human-Machine Learning Collaboration: A Pilot Study on Estimating Public Anxiety from Speech.",
                    "Impact of the Size of Modules on Target Acquisition and Pursuit for Future Modular Shape-changing Physical User Interfaces.",
                    "Why Do I Have to Take Over Control? Evaluating Safe Handovers with Advance Notice and Explanations in HAD.",
                    "ML-PersRef: A Machine Learning-based Personalized Multimodal Fusion Approach for Referencing Outside Objects From a Moving Vehicle.",
                    "Advances in Multimodal Behavioral Analytics for Early Dementia Diagnosis: A Review.",
                    "ConAn: A Usable Tool for Multimodal Conversation Analysis.",
                    "Prediction of Interlocutors' Subjective Impressions Based on Functional Head-Movement Features in Group Meetings.",
                    "Inflation-Deflation Networks for Recognizing Head-Movement Functions in Face-to-Face Conversations.",
                    "Deep Transfer Learning for Recognizing Functional Interactions via Head Movements in Multiparty Conversations.",
                    "Investigating the Effect of Polarity in Auditory and Vibrotactile Displays Under Cognitive Load.",
                    "User Preferences for Calming Affective Haptic Stimuli in Social Settings.",
                    "Improving the Movement Synchrony Estimation with Action Quality Assessment in Children Play Therapy.",
                    "Learning Oculomotor Behaviors from Scanpath.",
                    "Multimodal Detection of Drivers Drowsiness and Distraction.",
                    "On the Transition of Social Interaction from In-Person to Online: Predicting Changes in Social Media Usage of College Students during the COVID-19 Pandemic based on Pre-COVID-19 On-Campus Colocation.",
                    "Head Matters: Explainable Human-centered Trait Prediction from Head Motion Dynamics.",
                    "An Automated Mutual Gaze Detection Framework for Social Behavior Assessment in Therapy for Children with Autism.",
                    "Design and Development of a Low-cost Device for Weight and Center of Gravity Simulation in Virtual Reality.",
                    "Inclusive Voice Interaction Techniques for Creative Object Positioning.",
                    "Interaction Modalities for Notification Signals in Augmented Reality.",
                    "PARA: Privacy Management and Control in Emerging IoT Ecosystems using Augmented Reality.",
                    "Feature Perception in Broadband Sonar Analysis - Using the Repertory Grid to Elicit Interface Designs to Support Human-Autonomy Teaming.",
                    "To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures.",
                    "Audiovisual Speech Synthesis using Tacotron2.",
                    "What's This? A Voice and Touch Multimodal Approach for Ambiguity Resolution in Voice Assistants.",
                    "Graph Capsule Aggregation for Unaligned Multimodal Sequences.",
                    "Cross-modal Assisted Training for Abnormal Event Recognition in Elevators.",
                    "Towards Automatic Narrative Coherence Prediction.",
                    "TaxoVec: Taxonomy Based Representation for Web User Profiling.",
                    "Approximating the Mental Lexicon from Clinical Interviews as a Support Tool for Depression Detection.",
                    "Long-Term, in-the-Wild Study of Feedback about Speech Intelligibility for K-12 Students Attending Class via a Telepresence Robot.",
                    "EyeMU Interactions: Gaze + IMU Gestures on Mobile Devices.",
                    "Multimodal User Satisfaction Recognition for Non-task Oriented Dialogue Systems.",
                    "Cross Lingual Video and Text Retrieval: A New Benchmark Dataset and Algorithm.",
                    "Interaction Techniques for 3D-positioning Objects in Mobile Augmented Reality.",
                    "Engagement Rewarded Actor-Critic with Conservative Q-Learning for Speech-Driven Laughter Backchannel Generation.",
                    "Knowing Where and What to Write in Automated Live Video Comments: A Unified Multi-Task Approach.",
                    "Tomato Dice: A Multimodal Device to Encourage Breaks During Work.",
                    "Looking for Laughs: Gaze Interaction with Laughter Pragmatics and Coordination.",
                    "Improved Speech Emotion Recognition using Transfer Learning and Spectrogram Augmentation.",
                    "Mass-deployable Smartphone-based Objective Hearing Screening with Otoacoustic Emissions.",
                    "ThermEarhook: Investigating Spatial Thermal Haptic Feedback on the Auricular Skin Area.",
                    "Gaze-based Multimodal Meaning Recovery for Noisy / Complex Environments.",
                    "Semi-supervised Visual Feature Integration for Language Models through Sentence Visualization.",
                    "Speech Guided Disentangled Visual Representation Learning for Lip Reading.",
                    "Enhancing Ultrasound Haptics with Parametric Audio Effects.",
                    "Perception of Ultrasound Haptic Focal Point Motion.",
                    "Intra- and Inter-Contrastive Learning for Micro-expression Action Unit Detection.",
                    "HEMVIP: Human Evaluation of Multiple Videos in Parallel.",
                    "Knowledge- and Data-Driven Models of Multimodal Trajectories of Public Speaking Anxiety in Real and Virtual Settings.",
                    "Predicting Gaze from Egocentric Social Interaction Videos and IMU Data.",
                    "An Interpretable Approach to Hateful Meme Detection.",
                    "Human-Guided Modality Informativeness for Affective States.",
                    "Direct Gaze Triggers Higher Frequency of Gaze Change: An Automatic Analysis of Dyads in Unstructured Conversation.",
                    "Online Study Reveals the Multimodal Effects of Discrete Auditory Cues in Moving Target Estimation Task.",
                    "DynGeoNet: Fusion Network for Micro-expression Spotting.",
                    "Earthquake Response Drill Simulator based on a 3-DOF Motion base in Augmented Reality.",
                    "States of Confusion: Eye and Head Tracking Reveal Surgeons' Confusion during Arthroscopic Surgery.",
                    "Personality Prediction with Cross-Modality Feature Projection.",
                    "Attention-based Multimodal Feature Fusion for Dance Motion Generation.",
                    "Multimodal Approach for Assessing Neuromotor Coordination in Schizophrenia Using Convolutional Neural Networks.",
                    "M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in Conversations.",
                    "Optimized Human-AI Decision Making: A Personal Perspective.",
                    "Dependability and Safety: Two Clouds in the Blue Sky of Multimodal Interaction.",
                    "Towards Sonification in Multimodal and User-friendlyExplainable Artificial Intelligence.",
                    "Photogrammetry-based VR Interactive Pedagogical Agent for K12 Education.",
                    "Assisted End-User Robot Programming.",
                    "Using Generative Adversarial Networks to Create Graphical User Interfaces for Video Games.",
                    "Natural Language Stage of Change Modelling for \"Motivationally-driven\" Weight Loss Support.",
                    "Understanding Personalised Auditory-Visual Associations in Multi-Modal Interactions.",
                    "Semi-Supervised Learning for Multimodal Speech and Emotion Recognition.",
                    "Development of an Interactive Human/Agent Loop using Multimodal Recurrent Neural Networks.",
                    "What If I Interrupt You.",
                    "Accessible Applications - Study and Design of User Interfaces to Support Users with Disabilities.",
                    "Web-ECA: A Web-based ECA Platform.",
                    "Multimodal Interaction in the Production Line - An OPC UA-based Framework for Injection Molding Machinery.",
                    "Haply 2diy: An Accessible Haptic Plateform Suitable for Remote Learning.",
                    "Combining Visual and Social Dialogue for Human-Robot Interaction.",
                    "Introducing an Integrated VR Sensor Suite and Cloud Platform.",
                    "NLP-guided Video Thin-slicing for Automated Scoring of Non-Cognitive, Behavioral Performance Tasks.",
                    "The EMPATHIC Virtual Coach: a demo.",
                    "Automated Assessment of Pain.",
                    "2nd Workshop on Social Affective Multimodal Interaction for Health (SAMIH).",
                    "Insights on Group and Team Dynamics.",
                    "CATS2021: International Workshop on Corpora And Tools for Social skills annotation.",
                    "3rd Workshop on Modeling Socio-Emotional and Cognitive Processes from Multimodal Data in the Wild.",
                    "2nd ICMI Workshop on Bridging Social Sciences and AI for Understanding Child Behaviour.",
                    "ASMMC21: The 6th International Workshop on Affective Social Multimedia Computing.",
                    "Workshop on Multimodal Affect and Aesthetic Experience.",
                    "Empowering Interactive Robots by Learning Through Multimodal Feedback Channel.",
                    "GENEA Workshop 2021: The 2nd Workshop on Generation and Evaluation of Non-verbal Behaviour for Embodied Agents.",
                    "Socially Informed AI for Healthcare: Understanding and Generating Multimodal Nonverbal Cues."
                ]
            }
        ]
    },
    {
        "year": "2020",
        "name": "22nd ICMI 2020",
        "info": "Virtual Event, The Netherlands",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2020",
                "sub_name": "ICMI '20: International Conference on Multimodal Interaction, Virtual Event, The Netherlands, October 25-29, 2020.",
                "count": 128,
                "papers": [
                    "From Hands to Brains: How Does Human Body Talk, Think and Interact in Face-to-Face Language Use?",
                    "Musical Multimodal Interaction: From Bodies to Ecologies.",
                    "Human-centered Multimodal Machine Intelligence.",
                    "A Multi-modal System to Assess Cognition in Children from their Physical Movements.",
                    "A Neural Architecture for Detecting User Confusion in Eye-tracking Data.",
                    "Analysis of Face-Touching Behavior in Large Scale Social Interaction Dataset.",
                    "Attention Sensing through Multimodal User Modeling in an Augmented Reality Guessing Game.",
                    "BreathEasy: Assessing Respiratory Diseases Using Mobile Multimodal Sensors.",
                    "Bring the Environment to Life: A Sonification Module for People with Visual Impairments to Improve Situation Awareness.",
                    "Combining Auditory and Mid-Air Haptic Feedback for a Light Switch Button.",
                    "Depression Severity Assessment for Adolescents at High Risk of Mental Disorders.",
                    "Detecting Depression in Less Than 10 Seconds: Impact of Speaking Time on Depression Detection Sensitivity.",
                    "Did the Children Behave?: Investigating the Relationship Between Attachment Condition and Child Computer Interaction.",
                    "Dyadic Speech-based Affect Recognition using DAMI-P2C Parent-child Multimodal Interaction Dataset.",
                    "Early Prediction of Visitor Engagement in Science Museums with Multimodal Learning Analytics.",
                    "Effects of Visual Locomotion and Tactile Stimuli Duration on the Emotional Dimensions of the Cutaneous Rabbit Illusion.",
                    "Eliciting Emotion with Vibrotactile Stimuli Evocative of Real-World Sensations.",
                    "Enhancing Affect Detection in Game-Based Learning Environments with Multimodal Conditional Generative Modeling.",
                    "Estimating the Intensity of Facial Expressions Accompanying Feedback Responses in Multiparty Video-Mediated Communication.",
                    "Exploring Personal Memories and Video Content as Context for Facial Behavior in Predictions of Video-Induced Emotions.",
                    "Eye-Tracking to Predict User Cognitive Abilities and Performance for User-Adaptive Narrative Visualizations.",
                    "Facial Electromyography-based Adaptive Virtual Reality Gaming for Cognitive Training.",
                    "Facilitating Flexible Force Feedback Design with Feelix.",
                    "FeetBack: Augmenting Robotic Telepresence with Haptic Feedback on the Feet.",
                    "Fifty Shades of Green: Towards a Robust Measure of Inter-annotator Agreement for Continuous Signals.",
                    "FilterJoint: Toward an Understanding of Whole-Body Gesture Articulation.",
                    "Finally on Par?! Multimodal and Unimodal Interaction for Open Creative Design Tasks in Virtual Reality.",
                    "Force9: Force-assisted Miniature Keyboard on Smart Wearables.",
                    "Gesticulator: A framework for semantically-aware speech-driven gesture generation.",
                    "Gesture Enhanced Comprehension of Ambiguous Human-to-Robot Instructions.",
                    "Going with our Guts: Potentials of Wearable Electrogastrography (EGG) for Affect Detection.",
                    "Hand-eye Coordination for Textual Difficulty Detection in Text Summarization.",
                    "How Good is Good Enough?: The Impact of Errors in Single Person Action Classification on the Modeling of Group Interactions in Volleyball.",
                    "Incorporating Measures of Intermodal Coordination in Automated Analysis of Infant-Mother Interaction.",
                    "Influence of Electric Taste, Smell, Color, and Thermal Sensory Modalities on the Liking and Mediated Emotions of Virtual Flavor Perception.",
                    "Introducing Representations of Facial Affect in Automated Multimodal Deception Detection.",
                    "Is She Truly Enjoying the Conversation?: Analysis of Physiological Signals toward Adaptive Dialogue Systems.",
                    "Job Interviewer Android with Elaborate Follow-up Question Generation.",
                    "LASO: Exploiting Locomotive and Acoustic Signatures over the Edge to Annotate IMU Data for Human Activity Recognition.",
                    "LDNN: Linguistic Knowledge Injectable Deep Neural Network for Group Cohesiveness Understanding.",
                    "Mimicker-in-the-Browser: A Novel Interaction Using Mimicry to Augment the Browsing Experience.",
                    "Mitigating Biases in Multimodal Personality Assessment.",
                    "MMGatorAuth: A Novel Multimodal Dataset for Authentication Interactions in Gesture and Voice.",
                    "Modality Dropout for Improved Performance-driven Talking Faces.",
                    "MORSE: MultimOdal sentiment analysis for Real-life SEttings.",
                    "MSP-Face Corpus: A Natural Audiovisual Emotional Database.",
                    "Multimodal Automatic Coding of Client Behavior in Motivational Interviewing.",
                    "Multimodal Data Fusion based on the Global Workspace Theory.",
                    "Multimodal, Multiparty Modeling of Collaborative Problem Solving Performance.",
                    "PiHearts: Resonating Experiences of Self and Others Enabled by a Tangible Somaesthetic Design.",
                    "Predicting Video Affect via Induced Affection in the Wild.",
                    "Preserving Privacy in Image-based Emotion Recognition through User Anonymization.",
                    "Purring Wheel: Thermal and Vibrotactile Notifications on the Steering Wheel.",
                    "SmellControl: The Study of Sense of Agency in Smell.",
                    "Speaker-Invariant Adversarial Domain Adaptation for Emotion Recognition.",
                    "StrategicReading: Understanding Complex Mobile Reading Strategies via Implicit Behavior Sensing.",
                    "Studying Person-Specific Pointing and Gaze Behavior for Multimodal Referencing of Outside Objects from a Moving Vehicle.",
                    "Temporal Attention and Consistency Measuring for Video Question Answering.",
                    "The eyes know it: FakeET- An Eye-tracking Database to Understand Deepfake Perception.",
                    "The WoNoWa Dataset: Investigating the Transactive Memory System in Small Group Interactions.",
                    "Toward Adaptive Trust Calibration for Level 2 Driving Automation.",
                    "Toward Multimodal Modeling of Emotional Expressiveness.",
                    "Towards Engagement Recognition of People with Dementia in Care Settings.",
                    "Understanding Applicants' Reactions to Asynchronous Video Interviews Through Self-reports and Nonverbal Cues.",
                    "Using Emotions to Complement Multi-Modal Human-Robot Interaction in Urban Search and Rescue Scenarios.",
                    "\"Was that successful?\" On Integrating Proactive Meta-Dialogue in a DIY-Assistant using Multimodal Cues.",
                    "You Have a Point There: Object Selection Inside an Automobile Using Gaze, Head Pose and Finger Pointing.",
                    "A Comparison between Laboratory and Wearable Sensors in the Context of Physiological Synchrony.",
                    "Analyzing Nonverbal Behaviors along with Praising.",
                    "Automated Time Synchronization of Cough Events from Multimodal Sensors in Mobile Devices.",
                    "Conventional and Non-conventional Job Interviewing Methods: A Comparative Study in Two Countries.",
                    "Detection of Listener Uncertainty in Robot-Led Second Language Conversation Practice.",
                    "Effect of Modality on Human and Machine Scoring of Presentation Videos.",
                    "Examining the Link between Children's Cognitive Development and Touchscreen Interaction Patterns.",
                    "Gaze Tracker Accuracy and Precision Measurements in Virtual Reality Headsets.",
                    "Leniency to those who confess?: Predicting the Legal Judgement via Multi-Modal Analysis.",
                    "Multimodal Assessment of Oral Presentations using HMMs.",
                    "Multimodal Gated Information Fusion for Emotion Recognition from EEG Signals and Facial Behaviors.",
                    "OpenSense: A Platform for Multimodal Data Acquisition and Behavior Perception.",
                    "Personalized Modeling of Real-World Vocalizations from Nonverbal Individuals.",
                    "Predicting the Effectiveness of Systematic Desensitization Through Virtual Reality for Mitigating Public Speaking Anxiety.",
                    "Punchline Detection using Context-Aware Hierarchical Multimodal Fusion.",
                    "ROSMI: A Multimodal Corpus for Map-based Instruction-Giving.",
                    "The iCub Multisensor Datasets for Robot and Computer Vision Applications.",
                    "The Sensory Interactive Table: Exploring the Social Space of Eating.",
                    "Touch Recognition with Attentive End-to-End Model.",
                    "Automating Facilitation and Documentation of Collaborative Ideation Processes.",
                    "Detection of Micro-expression Recognition Based on Spatio-Temporal Modelling and Spatial Attention.",
                    "How to Complement Learning Analytics with Smartwatches?: Fusing Physical Activities, Environmental Context, and Learning Activities.",
                    "Multimodal Groups' Analysis for Automated Cohesion Estimation.",
                    "Multimodal Physiological Synchrony as Measure of Attentional Engagement.",
                    "Personalised Human Device Interaction through Context aware Augmented Reality.",
                    "Robot Assisted Diagnosis of Autism in Children.",
                    "Supporting Instructors to Provide Emotional and Instructional Scaffolding for English Language Learners through Biosensor-based Feedback.",
                    "Towards a Multimodal and Context-Aware Framework for Human Navigational Intent Inference.",
                    "Towards Multimodal Human-Like Characteristics and Expressive Visual Prosody in Virtual Agents.",
                    "Towards Real-Time Multimodal Emotion Recognition among Couples.",
                    "Zero-Shot Learning for Gesture Recognition.",
                    "Alfie: An Interactive Robot with Moral Compass.",
                    "FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment.",
                    "LieCatcher: Game Framework for Collecting Human Judgments of Deceptive Speech.",
                    "Spark Creativity by Speaking Enthusiastically: Communication Training using an E-Coach.",
                    "The AI-Medic: A Multimodal Artificial Intelligent Mentor for Trauma Surgery.",
                    "A Multi-Modal Approach for Driver Gaze Prediction to Remove Identity Bias.",
                    "Advanced Multi-Instance Learning Method with Multi-features Engineering and Conservative Optimization for Engagement Intensity Prediction.",
                    "EmotiW 2020: Driver Gaze, Group Emotion, Student Engagement and Physiological Signal based Challenges.",
                    "Extract the Gaze Multi-dimensional Information Analysis Driver Behavior.",
                    "Fusical: Multimodal Fusion for Video Sentiment.",
                    "Group Level Audio-Video Emotion Recognition Using Hybrid Networks.",
                    "Group-Level Emotion Recognition Using a Unimodal Privacy-Safe Non-Individual Approach.",
                    "Group-level Speech Emotion Recognition Utilising Deep Spectrum Features.",
                    "Implicit Knowledge Injectable Cross Attention Audiovisual Model for Group Emotion Recognition.",
                    "Multi-modal Fusion Using Spatio-temporal and Static Features for Group Emotion Recognition.",
                    "Multi-rate Attention Based GRU Model for Engagement Prediction.",
                    "Recognizing Emotion in the Wild using Multimodal Data.",
                    "X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild.",
                    "Bridging Social Sciences and AI for Understanding Child Behaviour.",
                    "International Workshop on Deep Video Understanding.",
                    "Face and Gesture Analysis for Health Informatics.",
                    "Workshop on Interdisciplinary Insights into Group and Team Dynamics.",
                    "Multisensory Approaches to Human-Food Interaction.",
                    "Multimodal Interaction in Psychopathology.",
                    "Modeling Socio-Emotional and Cognitive Processes from Multimodal Data in the Wild.",
                    "Speech, Voice, Text, and Meaning: A Multidisciplinary Approach to Interview Data through the use of digital tools.",
                    "Multimodal Affect and Aesthetic Experience.",
                    "First Workshop on Multimodal e-Coaches.",
                    "Social Affective Multimodal Interaction for Health.",
                    "The First International Workshop on Multi-Scale Movement Technologies."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2020c",
                "sub_name": "Companion Publication of the 2020 International Conference on Multimodal Interaction, ICMI Companion 2020, Virtual Event, The Netherlands, October, 2020.",
                "count": 103,
                "papers": [
                    "Gender Classification of Prepubescent Children via Eye Movements with Reading Stimuli.",
                    "Investigating LSTM for Micro-Expression Recognition.",
                    "Speech Emotion Recognition among Elderly Individuals using Multimodal Fusion and Transfer Learning.",
                    "Speech Emotion Recognition among Couples using the Peak-End Rule and Transfer Learning.",
                    "Music-Driven Animation Generation of Expressive Musical Gestures.",
                    "Engagement Analysis of ADHD Students using Visual Cues from Eye Tracker.",
                    "mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level Estimation.",
                    "User Expectations and Preferences to How Social Robots Render Text Messages with Emojis.",
                    "It's Not What They Play, It's What You Hear: Understanding Perceived vs. Induced Emotions in Hindustani Classical Music.",
                    "Toward Mathematical Representation of Emotion: A Deep Multitask Learning Method Based On Multimodal Recognition.",
                    "Neuroscience to Investigate Social Mechanisms Involved in Human-Robot Interactions.",
                    "Multimodal Self-Assessed Personality Prediction in the Wild.",
                    "Prediction of Shared Laughter for Human-Robot Dialogue.",
                    "The Influence of Blind Source Separation on Mixed Audio Speech and Music Emotion Recognition.",
                    "Using Physiological Cues to Determine Levels of Anxiety Experienced among Deaf and Hard of Hearing English Language Learners.",
                    "A Novel pseudo viewpoint based Holoscopic 3D Micro-gesture Recognition.",
                    "Physiological Synchrony, Stress and Communication of Paramedic Trainees During Emergency Response Training.",
                    "ET-CycleGAN: Generating Thermal Images from Images in the Visible Spectrum for Facial Emotion Recognition.",
                    "Visually Impaired User Experience using a 3D-Enhanced Facility Management System for Indoors Navigation.",
                    "Not All Errors Are Created Equal: Exploring Human Responses to Robot Errors with Varying Severity.",
                    "A Phonology-based Approach for Isolated Sign Production Assessment in Sign Language.",
                    "The Cross-modal Congruency Effect as an Objective Measure of Embodiment.",
                    "\"Was It You Who Stole 500 Rubles?\" - The Multimodal Deception Detection.",
                    "Deep Video Understanding of Character Relationships in Movies.",
                    "See me Speaking? Differentiating on Whether Words are Spoken On Screen or Off to Optimize Machine Dubbing.",
                    "Kinetics and Scene Features for Intent Detection.",
                    "Bodily Expression of Social Initiation Behaviors in ASC and non-ASC children: Mixed Reality vs. LEGO Game Play.",
                    "Using Object Tracking Techniques to Non-Invasively Measure Thoracic Rotation Range of Motion.",
                    "Enforcing Multilabel Consistency for Automatic Spatio-Temporal Assessment of Shoulder Pain Intensity.",
                    "Unsupervised Learning Method for Exploring Students' Mental Stress in Medical Simulation Training.",
                    "A Model of Team Trust in Human-Agent Teams.",
                    "Inferring Student Engagement in Collaborative Problem Solving from Visual Cues.",
                    "Modeling Dynamics of Task and Social Cohesion from the Group Perspective Using Nonverbal Motion Capture-based Features.",
                    "Group Performance Prediction with Limited Context.",
                    "Defining and Quantifying Conversation Quality in Spontaneous Interactions.",
                    "Machine Understanding of Emotion and Sentiment.",
                    "Bio-sensing of Environmental Distress for Walkable Built Environment.",
                    "Heart Rate Detection from the Supratrochlear Vessels using a Virtual Reality Headset integrated PPG Sensor.",
                    "Aesthetics in Hypermedia: Impact of Colour Harmony on Implicit Memory and User Experience.",
                    "Adaptive Audio Mixing for Enhancing Immersion in Augmented Reality Audio Games.",
                    "Action Modelling for Interaction and Analysis in Smart Sports and Physical Education.",
                    "Scalable Infrastructure for Efficient Real-Time Sports Analytics.",
                    "Autonomous and Remote Controlled Humanoid Robot for Fitness Training.",
                    "Physical Exercise Form Correction Using Neural Networks.",
                    "Climbing Activity Recognition and Measurement with Sensor Data Analysis.",
                    "Measuring Human Behaviour to Inform e-Coaching Actions.",
                    "Virtual Coaching for Older Adults at Home using SMART Goal Supported Behavior Change.",
                    "Coaching Older Adults: Persuasive and Multimodal Approaches to Coaching for Daily Living.",
                    "Transforming Rehabilitation to Virtually Supported Care - The vCare Project.",
                    "ISwimCoach: A Smart Coach guiding System for Assisting Swimmers Free Style Strokes: ISwimCoach.",
                    "Multimodal Conversational Agent for Older Adults' Behavioral Change.",
                    "Measuring and Fostering Engagement with Mental Health e-Coaches.",
                    "Trends & Methods in Chatbot Evaluation.",
                    "Co-Designing Flavor-Based Memory Cues with Older Adults.",
                    "Automatic Analysis of Facilitated Taste-liking.",
                    "The Influence of Emotion-Oriented Extrinsic Visual and Auditory Cues on Coffee Perception: A Virtual Reality Experiment.",
                    "An Accessible Tool to Measure Implicit Approach-Avoidance Tendencies Towards Food Outside the Lab.",
                    "Eating with an Artificial Commensal Companion.",
                    "Guess who's coming to dinner? Surveying Digital Commensality During Covid-19 Outbreak.",
                    "Augmentation of Perceived Sweetness in Sugar Reduced Cakes by Local Odor Display.",
                    "The Effect of Different Affective Arousal Levels on Taste Perception.",
                    "Multimodal Interactive Dining with the Sensory Interactive Table: Two Use Cases.",
                    "Eating Like an Astronaut: How Children Are Willing to Eat.",
                    "Eating Sound Dataset for 20 Food Types and Sound Classification Using Convolutional Neural Networks.",
                    "Ambient Pain Monitoring in Older Adults with Dementia to Improve Pain Management in Long-Term Care Facilities.",
                    "Automated Detection of Optimal DBS Device Settings.",
                    "Data Drive Development-Multimodal Measurement of Classroom Interaction.",
                    "Objective Measurement of Social Communication Behaviors in Children with Suspected ASD During the ADOS-2.",
                    "Mother-Infant Face-to-Face Intermodal Discrepancy and Risk.",
                    "MEMOS: A Multi-modal Emotion Stream Database for Temporal Spontaneous Emotional State Detection.",
                    "Emotion Recognition using EEG and Physiological Data for Robot-Assisted Rehabilitation Systems.",
                    "Is There 'ONE way' of Learning? A Data-driven Approach.",
                    "Multimodal Fuzzy Assessment for Robot Behavioral Adaptation in Educational Children-Robot Interaction.",
                    "Training Strategies to Handle Missing Modalities for Audio-Visual Expression Recognition.",
                    "Measuring Cognitive Load: Heart-rate Variability and Pupillometry Assessment.",
                    "The Wizard is Dead, Long live Data: towards Autonomous Social Behaviour using Data-driven Methods.",
                    "Assessment of Situation Awareness during Robotic Surgery using Multimodal Data.",
                    "Model-based Prediction of Exogeneous and Endogeneous Attention Shifts During an Everyday Activity.",
                    "SmartHelm: Towards Multimodal Detection of Attention in an Outdoor Augmented Reality Biking Scenario.",
                    "Gravity-Direction-Aware Joint Inter-Device Matching and Temporal Alignment between Camera and Wearable Sensors.",
                    "A Movement in Multiple Time Neural Network for Automatic Detection of Pain Behaviour.",
                    "Structuring Multi-Layered Musical Feedback for Digital Bodily Interaction: Two Approaches to Multi-layered Interactive Musical Feedback Systems.",
                    "A Computational Method to Automatically Detect the Perceived Origin of Full-Body Human Movement and its Propagation.",
                    "Speech, Voice, Text, And Meaning: A Multidisciplinary Approach to Interview Data through the use of digital tools.",
                    "Upskilling the Future Workforce Using AI and Affective Computing.",
                    "Preliminary Study of the Perception of Emotions Expressed by Virtual Agents in the Context of Parkinson's Disease.",
                    "Effectiveness of Virtual Reality Playback in Public Speaking Training.",
                    "Objective Prediction of Social Skills Level for Automated Social Skills Training Using Audio and Text Information.",
                    "Developing a Social Biofeedback Training System for Stress Management Training.",
                    "Analysis of Mood Changes and Facial Expressions during Cognitive Behavior Therapy through a Virtual Agent.",
                    "Children as Candidates to Verbal Nudging in a Human-robot Experiment.",
                    "Music Generation and Emotion Estimation from EEG Signals for Inducing Affective States.",
                    "Investigating the Influence of Sound Design for Inducing Anxiety in Virtual Public Speaking.",
                    "Towards Detecting Need for Empathetic Response in Motivational Interviewing.",
                    "Social Robot's Processing of Context-Sensitive Emotions in Child Care: A Dutch Use Case.",
                    "Speech and Gaze during Parent-Child Interactions: The Role of Conflict and Cooperation.",
                    "Using Markov Models and Classification to Understand Face Exploration Dynamics in Boys with Autism.",
                    "Eye Tracking in Human Interaction: Possibilities and Limitations.",
                    "Combining Clustering and Functionals based Acoustic Feature Representations for Classification of Baby Sounds.",
                    "Early Development Indicators Predict Speech Features of Autistic Children.",
                    "Automatic Recognition of Target Words in Infant-Directed Speech.",
                    "Instagram Use and the Well-Being of Adolescents: Using Deep Learning to Link Social Scientific Self-reports with Instagram Data Download Packages.",
                    "Speech Acquisition in Children with Typical and Atypical Development."
                ]
            }
        ]
    },
    {
        "year": "2019",
        "name": "21st ICMI 2019",
        "info": "Suzhou, China",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2019",
                "sub_name": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019.",
                "count": 87,
                "papers": [
                    "A Brief History of Intelligence.",
                    "Challenges of Multimodal Interaction in the Era of Human-Robot Coexistence.",
                    "Connecting Humans with Humans: Multimodal, Multilingual, Multiparty Mediation.",
                    "Socially-Aware User Interfaces: Can Genuine Sensitivity Be Learnt at all?",
                    "Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach.",
                    "Comparing Pedestrian Navigation Methods in Virtual Reality and Real Life.",
                    "Video and Text-Based Affect Analysis of Children in Play Therapy.",
                    "Facial Expression Recognition via Relation-based Conditional Generative Adversarial Network.",
                    "Continuous Emotion Recognition in Videos by Fusing Facial Expression, Head Pose and Eye Gaze.",
                    "Effect of Feedback on Users' Immediate Emotions: Analysis of Facial Expressions during a Simulated Target Detection Task.",
                    "Multimodal Analysis and Estimation of Intimate Self-Disclosure.",
                    "A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression Capabilities.",
                    "To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations.",
                    "Multitask Prediction of Exchange-level Annotations for Multimodal Dialogue Systems.",
                    "Multimodal Learning for Identifying Opportunities for Empathetic Responses.",
                    "Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics.",
                    "VisualTouch: Enhancing Affective Touch Communication with Multi-modality Stimulation.",
                    "TouchPhoto: Enabling Independent Picture Taking and Understanding for Visually-Impaired Users.",
                    "Creativity Support and Multimodal Pen-based Interaction.",
                    "Motion Eavesdropper: Smartwatch-based Handwriting Recognition Using Deep Learning.",
                    "Predicting Cognitive Load in an Emergency Simulation Based on Behavioral and Physiological Measures.",
                    "Driving Anomaly Detection with Conditional Generative Adversarial Network using Physiological and CAN-Bus Data.",
                    "Controlling for Confounders in Multimodal Emotion Classification via Adversarial Learning.",
                    "Multimodal Classification of EEG During Physical Activity.",
                    "\"Paint that object yellow\": Multimodal Interaction to Enhance Creativity During Design Tasks in VR.",
                    "VCMNet: Weakly Supervised Learning for Automatic Infant Vocalisation Maturity Analysis.",
                    "Evidence for Communicative Compensation in Debt Advice with Reduced Multimodality.",
                    "Speaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models.",
                    "Smooth Turn-taking by a Robot Using an Online Continuous Model to Generate Turn-taking Cues.",
                    "Towards Automatic Detection of Misinformation in Online Medical Videos.",
                    "Modeling Team-level Multimodal Dynamics during Multiparty Collaboration.",
                    "Smile and Laugh Dynamics in Naturalistic Dyadic Interactions: Intensity Levels, Sequences and Roles.",
                    "Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions.",
                    "Emergent Leadership Detection Across Datasets.",
                    "A Multimodal Robot-Driven Meeting Facilitation System for Group Decision-Making Sessions.",
                    "What's behind a choice? Understanding Modality Choices under Changing Environmental Conditions.",
                    "Modeling Emotion Influence Using Attention-based Graph Convolutional Recurrent Network.",
                    "Evaluation of Ultrasound Haptics as a Supplementary Feedback Cue for Grasping in Virtual Environments.",
                    "Understanding the Attention Demand of Touch and Tangible Interaction on a Composite Task.",
                    "TouchGazePath: Multimodal Interaction with Touch and Gaze Path for Secure Yet Efficient PIN Entry.",
                    "WiBend: Wi-Fi for Sensing Passive Deformable Surfaces.",
                    "ElderReact: A Multimodal Dataset for Recognizing Emotional Response in Aging Adults.",
                    "Unsupervised Deep Fusion Cross-modal Hashing.",
                    "DIF : Dataset of Perceived Intoxicated Faces for Drunk Person Identification.",
                    "Generative Model of Agent's Behaviors in Human-Agent Interaction.",
                    "Improved Visual Focus of Attention Estimation and Prosodic Features for Analyzing Group Interactions.",
                    "DeepReviewer: Collaborative Grammar and Innovation Neural Network for Automatic Paper Review.",
                    "CorrFeat: Correlation-based Feature Extraction Algorithm using Skin Conductance and Pupil Diameter for Emotion Recognition.",
                    "Multimodal Behavioral Markers Exploring Suicidal Intent in Social Media Videos.",
                    "Estimating Uncertainty in Task-Oriented Dialogue.",
                    "Determining Iconic Gesture Forms based on Entity Image Representation.",
                    "Interaction Process Label Recognition in Group Discussion.",
                    "Exploring Transfer Learning between Scripted and Spontaneous Speech for Emotion Recognition.",
                    "Engagement Modeling in Dyadic Interaction.",
                    "Detecting Temporal Phases of Anxiety in The Wild: Toward Continuously Adaptive Self-Regulation Technologies.",
                    "Multimodal Machine Learning for Interactive Mental Health Therapy.",
                    "Tailoring Motion Recognition Systems to Children's Motions.",
                    "Multi-modal Fusion Methods for Robust Emotion Recognition using Body-worn Physiological Sensors in Mobile Environments.",
                    "Communicative Signals and Social Contextual Factors in Multimodal Affect Recognition.",
                    "Co-located Collaboration Analytics.",
                    "Coalescing Narrative and Dialogue for Grounded Pose Forecasting.",
                    "Attention-driven Interaction Systems for Augmented Reality.",
                    "Multimodal Driver Interaction with Gesture, Gaze and Speech.",
                    "The Dyslexperience: Use of Projection Mapping to Simulate Dyslexia.",
                    "A Real-Time Scene Recognition System Based on RGB-D Video Streams.",
                    "Hang Out with the Language Assistant.",
                    "A Searching and Automatic Video Tagging Tool for Events of Interest during Volleyball Training Sessions.",
                    "Seeing Is Believing but Feeling Is the Truth: Visualising Mid-Air Haptics in Oil Baths and Lightboxes.",
                    "Chemistry Pods: A Mutlimodal Real Time and Retrospective Tool for the Classroom.",
                    "A Proxemics Measurement Tool Integrated into VAIF and Unity.",
                    "Transfer Learning Methods for Spoken Language Understanding.",
                    "Streamlined Decoder for Chinese Spoken Language Understanding.",
                    "CATSLU: The 1st Chinese Audio-Textual Spoken Language Understanding Challenge.",
                    "Multi-Classification Model for Spoken Language Understanding.",
                    "Robust Spoken Language Understanding with Acoustic and Domain Knowledge.",
                    "Spotting Visual Keywords from Temporal Sliding Windows.",
                    "Deep Audio-visual System for Closed-set Word-level Speech Recognition.",
                    "EmotiW 2019: Automatic Emotion, Engagement and Cohesion Prediction Tasks.",
                    "Bootstrap Model Ensemble and Rank Loss for Engagement Intensity Regression.",
                    "Exploring Regularizations with Face, Body and Image Cues for Group Cohesion Prediction.",
                    "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition.",
                    "Engagement Intensity Prediction withFacial Behavior Features.",
                    "Group-level Cohesion Prediction using Deep Learning Models with A Multi-stream Hybrid Network.",
                    "Automatic Group Cohesiveness Detection With Multi-modal Features.",
                    "Multi-feature and Multi-instance Learning with Anti-overfitting Strategy for Engagement Intensity Prediction.",
                    "Bi-modality Fusion for Emotion Recognition in the Wild.",
                    "Multi-Attention Fusion Network for Video-based Emotion Recognition."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2019a",
                "sub_name": "Adjunct of the 2019 International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019.",
                "count": 15,
                "papers": [
                    "Fusing Dialogue and Gaze From Discussions of 2D and 3D Scenes.",
                    "Multimodal Anticipated versus Actual Perceptual Reactions.",
                    "Measuring Affective Sharing between Two People by EEG Hyperscanning.",
                    "Detecting Syntactic Violations from Single-trial EEG using Recurrent Neural Networks.",
                    "Detecting Dementia from Face in Human-Agent Interaction.",
                    "Multimodal Biometric Authentication for VR/AR using EEG and Eye Tracking.",
                    "An Approach to Reading Assistance with Eye Tracking Data and Text Features.",
                    "Evaluation of Dominant and Non-Dominant Hand Movements For Volleyball Action Modelling.",
                    "Are Humans Biased in Assessment of Video Interviews?",
                    "Lemusade: Make Lemonade Using Music.",
                    "Interactive Upper Limb Training Device for Arm-Reaching and Finger Pointing Exercise.",
                    "Multimodal Assessment on Teaching Skills via Neural Networks.",
                    "Floor Apportionment Function of Speaker's Gaze in Grounding Acts.",
                    "Real-Time Multimodal Classification of Internal and External Attention.",
                    "Sensory Substitution Device Stabilizing Human Voice Production."
                ]
            }
        ]
    },
    {
        "year": "2018",
        "name": "20th ICMI 2018",
        "info": "Boulder, CO, USA",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2018",
                "sub_name": "Proceedings of the 2018 on International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, October 16-20, 2018.",
                "count": 106,
                "papers": [
                    "A Multimodal Approach to Understanding Human Vocal Expressions and Beyond.",
                    "Using Technology for Health and Wellbeing.",
                    "Reinforcing, Reassuring, and Roasting: The Forms and Functions of the Human Smile.",
                    "Put That There: 20 Years of Research on Multimodal Interaction.",
                    "Multimodal Dialogue Management for Multiparty Interaction with Infants.",
                    "Predicting Group Performance in Task-Based Interaction.",
                    "Multimodal Modeling of Coordination and Coregulation Patterns in Speech Rate during Triadic Collaborative Problem Solving.",
                    "Analyzing Gaze Behavior and Dialogue Act during Turn-taking for Estimating Empathy Skill Level.",
                    "Automated Affect Detection in Deep Brain Stimulation for Obsessive-Compulsive Disorder: A Pilot Study.",
                    "Smell-O-Message: Integration of Olfactory Notifications into a Messaging Application to Improve Users' Performance.",
                    "Generating fMRI-Enriched Acoustic Vectors using a Cross-Modality Adversarial Network for Emotion Recognition.",
                    "Adaptive Review for Mobile MOOC Learning via Multimodal Physiological Signal Sensing - A Longitudinal Study.",
                    "Olfactory Display Prototype for Presenting and Sensing Authentic and Synthetic Odors.",
                    "Evaluation of Real-time Deep Learning Turn-taking Models for Multiple Dialogue Scenarios.",
                    "Ten Opportunities and Challenges for Advancing Student-Centered Multimodal Learning Analytics.",
                    "If You Ask Nicely: A Digital Assistant Rebuking Impolite Voice Commands.",
                    "Detecting User's Likes and Dislikes for a Virtual Negotiating Agent.",
                    "Attention-based Audio-Visual Fusion for Robust Automatic Speech Recognition.",
                    "Smart Arse: Posture Classification with Textile Sensors in Trousers.",
                    "!FTL, an Articulation-Invariant Stroke Gesture Recognizer with Controllable Position, Scale, and Rotation Invariances.",
                    "Pen + Mid-Air Gestures: Eliciting Contextual Gestures.",
                    "Hand, Foot or Voice: Alternative Input Modalities for Touchless Interaction in the Medical Domain.",
                    "How to Shape the Humor of a Robot - Social Behavior Adaptation Based on Reinforcement Learning.",
                    "Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction.",
                    "Toward Objective, Multifaceted Characterization of Psychotic Disorders: Lexical, Structural, and Disfluency Markers of Spoken Language.",
                    "Multimodal Interaction Modeling of Child Forensic Interviewing.",
                    "Multimodal Continuous Turn-Taking Prediction Using Multiscale RNNs.",
                    "Estimating Visual Focus of Attention in Multiparty Meetings using Deep Convolutional Neural Networks.",
                    "Detecting Deception and Suspicion in Dyadic Game Interactions.",
                    "Looking Beyond a Clever Narrative: Visual Context and Attention are Primary Drivers of Affect in Video Advertisements.",
                    "Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals.",
                    "Population-specific Detection of Couples' Interpersonal Conflict using Multi-task Learning.",
                    "I Smell Trouble: Using Multiple Scents To Convey Driving-Relevant Information.",
                    "\"Honey, I Learned to Talk\": Multimodal Fusion for Behavior Analysis.",
                    "TapTag: Assistive Gestural Interactions in Social Media on Touchscreens for Older Adults.",
                    "Gazeover - Exploring the UX of Gaze-triggered Affordance Communication for GUI Elements.",
                    "Dozing Off or Thinking Hard?: Classifying Multi-dimensional Attentional States in the Classroom from Video.",
                    "Sensing Arousal and Focal Attention During Visual Interaction.",
                    "Path Word: A Multimodal Password Entry Method for Ad-hoc Authentication Based on Digits' Shape and Smooth Pursuit Eye Movements.",
                    "Towards Attentive Speed Reading on Small Screen Wearable Devices.",
                    "Understanding Mobile Reading via Camera Based Gaze Tracking and Kinematic Touch Modeling.",
                    "Inferring User Intention using Gaze in Vehicles.",
                    "EyeLinks: A Gaze-Only Click Alternative for Heterogeneous Clickables.",
                    "EEG-based Evaluation of Cognitive Workload Induced by Acoustic Parameters for Data Sonification.",
                    "A Multimodal Approach for Predicting Changes in PTSD Symptom Severity.",
                    "Floor Apportionment and Mutual Gazes in Native and Second-Language Conversation.",
                    "Estimating Head Motion from Egocentric Vision.",
                    "A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis.",
                    "Multimodal Analysis of Client Behavioral Change Coding in Motivational Interviewing.",
                    "End-to-end Learning for 3D Facial Animation from Speech.",
                    "Joint Discrete and Continuous Emotion Prediction Using Ensemble and End-to-End Approaches.",
                    "The Multimodal Dataset of Negative Affect and Aggression: A Validation Study.",
                    "Keep Me in the Loop: Increasing Operator Situation Awareness through a Conversational Multimodal Interface.",
                    "Simultaneous Multimodal Access to Wheelchair and Computer for People with Tetraplegia.",
                    "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection.",
                    "Enhancing Multiparty Cooperative Movements: A Robotic Wheelchair that Assists in Predicting Next Actions.",
                    "Multimodal Representation of Advertisements Using Segment-level Autoencoders.",
                    "Survival at the Museum: A Cooperation Experiment with Emotionally Expressive Virtual Characters.",
                    "Human, Chameleon or Nodding Dog?",
                    "A Generative Approach for Dynamically Varying Photorealistic Facial Expressions in Human-Agent Interactions.",
                    "Predicting ADHD Risk from Touch Interaction Data.",
                    "Exploring the Design of Audio-Kinetic Graphics for Education.",
                    "RainCheck: Overcoming Capacitive Interference Caused by Rainwater on Smartphones.",
                    "Multimodal Local-Global Ranking Fusion for Emotion Recognition.",
                    "Improving Object Disambiguation from Natural Language using Empirical Models.",
                    "Tactile Sensitivity to Distributed Patterns in a Palm.",
                    "Listening Skills Assessment through Computer Agents.",
                    "Using Data-Driven Approach for Modeling Timing Parameters of American Sign Language.",
                    "Unobtrusive Analysis of Group Interactions without Cameras.",
                    "Multimodal and Context-Aware Interaction in Augmented Reality for Active Assistance.",
                    "Interpretable Multimodal Deception Detection in Videos.",
                    "Attention Network for Engagement Prediction in the Wild.",
                    "Data Driven Non-Verbal Behavior Generation for Humanoid Robots.",
                    "Multi-Modal Multi sensor Interaction between Human andHeterogeneous Multi-Robot System.",
                    "Responding with Sentiment Appropriate for the User's Current Sentiment in Dialog as Inferred from Prosody and Gaze Patterns.",
                    "Strike A Pose: Capturing Non-Verbal Behaviour with Textile Sensors.",
                    "Large Vocabulary Continuous Audio-Visual Speech Recognition.",
                    "Multimodal Teaching and Learning Analytics for Classroom and Online Educational Settings.",
                    "Modeling Empathy in Embodied Conversational Agents: Extended Abstract.",
                    "EVA: A Multimodal Argumentative Dialogue System.",
                    "Online Privacy-Safe Engagement Tracking System.",
                    "Multimodal Control of Lighter-Than-Air Agents.",
                    "MIRIAM: A Multimodal Interface for Explaining the Reasoning Behind Actions of Remote Autonomous Systems.",
                    "EAT -: The ICMI 2018 Eating Analysis and Tracking Challenge.",
                    "SAAMEAT: Active Feature Transformation and Selection Methods for the Recognition of User Eating Conditions.",
                    "Exploring A New Method for Food Likability Rating Based on DT-CWT Theory.",
                    "Deep End-to-End Representation Learning for Food Type Recognition from Speech.",
                    "Functional-Based Acoustic Group Feature Selection for Automatic Recognition of Eating Condition.",
                    "Video-based Emotion Recognition Using Deeply-Supervised Neural Networks.",
                    "An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets.",
                    "Deep Recurrent Multi-instance Learning with Spatio-temporal Features for Engagement Intensity Prediction.",
                    "Automatic Engagement Prediction with GAP Feature.",
                    "Predicting Engagement Intensity in the Wild Using Temporal Convolutional Network.",
                    "An Attention Model for Group-Level Emotion Recognition.",
                    "An Ensemble Model Using Face and Body Tracking for Engagement Detection.",
                    "Group-Level Emotion Recognition using Deep Models with A Four-stream Hybrid Network.",
                    "Multi-Feature Based Emotion Recognition for Video Clips.",
                    "Group-Level Emotion Recognition Using Hybrid Deep Models Based on Faces, Scenes, Skeletons and Visual Attentions.",
                    "Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues.",
                    "Multiple Spatio-temporal Feature Learning for Video-based Emotion Recognition in the Wild.",
                    "EmotiW 2018: Audio-Video, Student Engagement and Group-Level Affect Prediction.",
                    "3rd International Workshop on Multisensory Approaches to Human-Food Interaction.",
                    "Group Interaction Frontiers in Technology.",
                    "Modeling Cognitive Processes from Multimodal Signals.",
                    "Human-Habitat for Health (H3): Human-habitat Multimodal Interaction for Promoting Health and Well-being in the Internet of Things Era.",
                    "International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction (Workshop Summary)."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2018a",
                "sub_name": "Proceedings of the International Conference on Multimodal Interaction: Adjunct, ICMI 2018, Boulder, CO, USA, October 16-20, 2018.",
                "count": 11,
                "papers": [
                    "Detecting head movements in video-recorded dyadic conversations.",
                    "Estimating interviewee's willingness in multimodal human robot interview interaction.",
                    "Sarcasm detection on Facebook: a supervised learning approach.",
                    "Constructionist steps towards an autonomously empathetic system.",
                    "Real-time stress assessment through PPG sensor for VR biofeedback.",
                    "Multimodal prediction of the audience's impression in political debates.",
                    "Distinction of stress and non-stress tasks using facial action units.",
                    "Effects of face and voice deformation on participant emotion in video-mediated communication.",
                    "Investigating the generalizability of EEG-based cognitive load estimation across visualizations.",
                    "Using virtual reality to control swarms of autonomous agents.",
                    "Investigating the dimensions of conversational agents' social competence using objective neurophysiological measurements."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2018mcpmd",
                "sub_name": "Proceedings of the Workshop on Modeling Cognitive Processes from Multimodal Data, MCPMD@ICMI 2018, Boulder, CO, USA, October 16, 2018.",
                "count": 13,
                "papers": [
                    "Predicting group satisfaction in meeting discussions.",
                    "Multimodal approach for cognitive task performance prediction from body postures, facial expressions and EEG signal.",
                    "Workload-driven modulation of mixed-reality robot-human communication.",
                    "Symptoms of cognitive load in interactions with a dialogue system.",
                    "Histogram of oriented velocities for eye movement detection.",
                    "Estimating mental load in passive and active tasks from pupil and gaze changes using bayesian surprise.",
                    "Investigating static and sequential models for intervention-free selection using multimodal data of EEG and eye tracking.",
                    "Overlooking: the nature of gaze behavior and anomaly detection in expert dentists.",
                    "Rule-based learning for eye movement type detection.",
                    "Integrating non-invasive neuroimaging and computer log data to improve understanding of cognitive processes.",
                    "Multimer: validating multimodal, cognitive data in the city: towards a model of how the urban environment influences streetscape users.",
                    "The role of emotion in problem solving: first results from observing chess.",
                    "Discovering digital representations for remembered episodes from lifelog data."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2018ma3hmi",
                "sub_name": "Proceedings of the 4th International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction, MA3HMI@ICMI 2018, Boulder, CO, USA, October 16, 2018.",
                "count": 8,
                "papers": [
                    "Analysis of the Effect of Agent's Embodiment and Gaze Amount on Personality Perception.",
                    "User Affect and No-Match Dialogue Scenarios: An Analysis of Facial Expression.",
                    "Exploring Siamese Neural Network Architectures for Preserving Speaker Identity in Speech Emotion Classification.",
                    "Extracting Interpersonal Stance from Vocal Signals.",
                    "A Pilot Study on Adaptive Gesture Use in Interaction with Non-native Listeners.",
                    "Recognition of Human Movement Patterns during a Human-Agent Interaction.",
                    "Multimodal Reference Resolution In Collaborative Assembly Tasks.",
                    "PauseCode: Computational Conversation Timing Analysis."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2018gift",
                "sub_name": "Proceedings of the Group Interaction Frontiers in Technology Workshop, GIFT@ICMI 2018, Boulder, CO, USA, October 16, 2018.",
                "count": 10,
                "papers": [
                    "Affective Dynamics and Control in Group Processes.",
                    "The Group Affect and Performance (GAP) Corpus.",
                    "Improving Temporal Interpolation of Head and Body Pose using Gaussian Process Regression in a Matrix Completion Setting.",
                    "Multimodal Analysis of Group Attitudes Towards Meeting Management.",
                    "Interpreting Models of Social Group Interactions in Meetings with Probabilistic Model Checking.",
                    "Adopting Functional Roles for Improving Participants' Communication Skill in Group Discussion Conversation.",
                    "Predicting Student Performance Based on Eye Gaze During Collaborative Problem Solving.",
                    "Kid Space: Interactive Learning in a Smart Environment.",
                    "Fusing Verbal and Nonverbal Information for Extractive Meeting Summarization.",
                    "Using Parallel Episodes of Speech to Represent and Identify Interaction Dynamics for Group Meetings."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2018mhfi",
                "sub_name": "Proceedings of the 3rd International Workshop on Multisensory Approaches to Human-Food Interaction, MHFI@ICMI 2018, Boulder, CO, USA, October 16, 2018.",
                "count": 8,
                "papers": [
                    "TasteBud: Bring Taste Back into the Game.",
                    "Towards Multisensory Storytelling with Taste and Flavor.",
                    "Drink-O-Mender: An Adaptive Robotic Drink Adviser.",
                    "\"Eat What You Want and Be Healthy!\": Comfort Food Effects: Human-Food Interaction in View of Celebratory Technology.",
                    "The Virtual Cafeteria: An Immersive Environment for Interactive Food Portion-Size Education.",
                    "Eliciting User Food Preferences in terms of Taste and Texture in Spoken Dialogue Systems.",
                    "Introducing Flavorlens: A Social Media Platform for Sharing Dish Observations.",
                    "Tasty Art: 'The Scream' as a Burger..."
                ]
            }
        ]
    },
    {
        "year": "2017",
        "name": "19th ICMI 2017",
        "info": "Glasgow, UK",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2017",
                "sub_name": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ICMI 2017, Glasgow, United Kingdom, November 13 - 17, 2017.",
                "count": 118,
                "papers": [
                    "Gastrophysics: using technology to enhance the experience of food and drink (keynote).",
                    "Collaborative robots: from action and interaction to collaboration (keynote).",
                    "Situated conceptualization: a framework for multimodal interaction (keynote).",
                    "Steps towards collaborative multimodal dialogue (sustained contribution award).",
                    "Tablets, tabletops, and smartphones: cross-platform comparisons of children's touchscreen interactions.",
                    "Toward an efficient body expression recognition based on the synthesis of a neutral movement.",
                    "Interactive narration with a child: impact of prosody and facial expressions.",
                    "Comparing human and machine recognition of children's touchscreen stroke gestures.",
                    "Virtual debate coach design: assessing multimodal argumentation performance.",
                    "Predicting the distribution of emotion perception: capturing inter-rater variability.",
                    "Automatically predicting human knowledgeability through non-verbal cues.",
                    "Pooling acoustic and lexical features for the prediction of valence.",
                    "Hand-to-hand: an intermanual illusion of movement.",
                    "An investigation of dynamic crossmodal instantiation in TUIs.",
                    "\"Stop over there\": natural gesture and speech interaction for non-critical spontaneous intervention in autonomous driving.",
                    "Pre-touch proxemics: moving the design space of touch targets from still graphics towards proxemic behaviors.",
                    "Freehand grasping in mixed reality: analysing variation during transition phase of interaction.",
                    "Rhythmic micro-gestures: discreet interaction on-the-go.",
                    "Evaluation of psychoacoustic sound parameters for sonification.",
                    "Utilising natural cross-modal mappings for visual control of feature-based sound synthesis.",
                    "Automatic classification of auto-correction errors in predictive text entry based on EEG and context information.",
                    "Cumulative attributes for pain intensity estimation.",
                    "Towards the use of social interaction conventions as prior for gaze model adaptation.",
                    "Multimodal sentiment analysis with word-level fusion and reinforcement learning.",
                    "IntelliPrompter: speech-based dynamic note display interface for oral presentations.",
                    "Head and shoulders: automatic error detection in human-robot interaction.",
                    "The reliability of non-verbal cues for situated reference resolution and their interplay with language: implications for human robot interaction.",
                    "Do you speak to a human or a virtual agent? automatic analysis of user's social cues during mediated communication.",
                    "Estimating verbal expressions of task and social cohesion in meetings by quantifying paralinguistic mimicry.",
                    "Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks.",
                    "Automatic assessment of communication skill in non-conventional interview settings: a comparative study.",
                    "Low-intrusive recognition of expressive movement qualities.",
                    "Digitising a medical clerking system with multimodal interaction support.",
                    "GazeTap: towards hands-free interaction in the operating room.",
                    "Boxer: a multimodal collision technique for virtual objects.",
                    "Trust triggers for multimodal command and control interfaces.",
                    "TouchScope: a hybrid multitouch oscilloscope interface.",
                    "A multimodal system to characterise melancholia: cascaded bag of words approach.",
                    "Crowdsourcing ratings of caller engagement in thin-slice videos of human-machine dialog: benefits and pitfalls.",
                    "Modelling fusion of modalities in multimodal interactive systems with MMMM.",
                    "Temporal alignment using the incremental unit framework.",
                    "Multimodal gender detection.",
                    "How may I help you? behavior and impressions in hospitality service encounters.",
                    "Tracking liking state in brain activity while watching multiple movies.",
                    "Does serial memory of locations benefit from spatially congruent audiovisual stimuli? investigating the effect of adding spatial sound to visuospatial sequences.",
                    "ZSGL: zero shot gestural learning.",
                    "Markov reward models for analyzing group interaction.",
                    "Analyzing first impressions of warmth and competence from observable nonverbal cues in expert-novice interactions.",
                    "The NoXi database: multimodal recordings of mediated novice-expert interactions.",
                    "Head-mounted displays as opera glasses: using mixed-reality to deliver an egalitarian user experience during live events.",
                    "Analyzing gaze behavior during turn-taking for estimating empathy skill level.",
                    "Text based user comments as a signal for automatic language identification of online videos.",
                    "Gender and emotion recognition with implicit user signals.",
                    "Animating the adelino robot with ERIK: the expressive robotics inverse kinematics.",
                    "Automatic detection of pain from spontaneous facial expressions.",
                    "Evaluating content-centric vs. user-centric ad affect recognition.",
                    "A domain adaptation approach to improve speaker turn embedding using face representation.",
                    "Computer vision based fall detection by a convolutional neural network.",
                    "Predicting meeting extracts in group discussions using multimodal convolutional neural networks.",
                    "The relationship between task-induced stress, vocal changes, and physiological state during a dyadic team task.",
                    "Meyendtris: a hands-free, multimodal tetris clone using eye tracking and passive BCI for intuitive neuroadaptive gaming.",
                    "AMHUSE: a multimodal dataset for HUmour SEnsing.",
                    "GazeTouchPIN: protecting sensitive data on mobile devices using secure multimodal authentication.",
                    "Multi-task learning of social psychology assessments and nonverbal features for automatic leadership identification.",
                    "Multimodal analysis of vocal collaborative search: a public corpus and results.",
                    "UE-HRI: a new dataset for the study of user engagement in spontaneous human-robot interactions.",
                    "Mining a multimodal corpus of doctor's training for virtual patient's feedbacks.",
                    "Multimodal affect recognition in an interactive gaming environment using eye tracking and speech signals.",
                    "Multimodal interaction in classrooms: implementation of tangibles in integrated music and math lessons.",
                    "Web-based interactive media authoring system with multimodal interaction.",
                    "Textured surfaces for ultrasound haptic displays.",
                    "Rapid development of multimodal interactive systems: a demonstration of platform for situated intelligence.",
                    "MIRIAM: a multimodal chat-based interface for autonomous systems.",
                    "SAM: the school attachment monitor.",
                    "The Boston Massacre history experience.",
                    "Demonstrating TouchScope: a hybrid multitouch oscilloscope interface.",
                    "The MULTISIMO multimodal corpus of collaborative interactions.",
                    "Using mobile virtual reality to empower people with hidden disabilities to overcome their barriers.",
                    "Bot or not: exploring the fine line between cyber and human identity.",
                    "Modulating the non-verbal social signals of a humanoid robot.",
                    "Thermal in-car interaction for navigation.",
                    "AQUBE: an interactive music reproduction system for aquariums.",
                    "Real-time mixed-reality telepresence via 3D reconstruction with HoloLens and commodity depth sensors.",
                    "Evaluating robot facial expressions.",
                    "Bimodal feedback for in-car mid-air gesture interaction.",
                    "A modular, multimodal open-source virtual interviewer dialog agent.",
                    "Wearable interactive display for the local positioning system (LPS).",
                    "From individual to group-level emotion recognition: EmotiW 5.0.",
                    "Multi-modal emotion recognition using semi-supervised learning and multiple neural networks in the wild.",
                    "Modeling multimodal cues in a deep learning-based framework for emotion recognition in the wild.",
                    "Group-level emotion recognition using transfer learning from face identification.",
                    "Group emotion recognition with individual facial emotion CNNs and global image based CNNs.",
                    "Learning supervised scoring ensemble for emotion recognition in the wild.",
                    "Group emotion recognition in the wild by combining deep neural networks for facial expression classification and scene-context analysis.",
                    "Temporal multimodal fusion for video emotion classification in the wild.",
                    "Audio-visual emotion recognition using deep transfer learning and multiple temporal models.",
                    "Multi-level feature fusion for group-level emotion recognition.",
                    "A new deep-learning framework for group emotion recognition.",
                    "Emotion recognition in the wild using deep neural networks and Bayesian classifiers.",
                    "Emotion recognition with multimodal features and temporal models.",
                    "Group-level emotion recognition using deep models on image scene, faces, and skeletons.",
                    "Towards designing speech technology based assistive interfaces for children's speech therapy.",
                    "Social robots for motivation and engagement in therapy.",
                    "Immersive virtual eating and conditioned food responses.",
                    "Towards edible interfaces: designing interactions with food.",
                    "Towards a computational model for first impressions generation.",
                    "A decentralised multimodal integration of social signals: a bio-inspired approach.",
                    "Human-centered recognition of children's touchscreen gestures.",
                    "Cross-modality interaction between EEG signals and facial expression.",
                    "Hybrid models for opinion analysis in speech interactions.",
                    "Evaluating engagement in digital narratives from facial data.",
                    "Social signal extraction from egocentric photo-streams.",
                    "Multimodal language grounding for improved human-robot collaboration: exploring spatial semantic representations in the shared space of attention.",
                    "ISIAA 2017: 1st international workshop on investigating social interactions with artificial agents (workshop summary).",
                    "WOCCI 2017: 6th international workshop on child computer interaction (workshop summary).",
                    "MIE 2017: 1st international workshop on multimodal interaction for education (workshop summary).",
                    "Playlab: telling stories with technology (workshop summary).",
                    "MHFI 2017: 2nd international workshop on multisensorial approaches to human-food interaction (workshop summary)."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2017isiaa",
                "sub_name": "Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents, ISIAA@ICMI 2017, Glasgow, United Kingdom, November 13, 2017.",
                "count": 20,
                "papers": [
                    "How do artificial agents think?",
                    "Body language without a body: nonverbal communication in technology mediated settings.",
                    "Dialogue management in task-oriented dialogue systems.",
                    "Greta: a conversing socio-emotional agent.",
                    "Challenges for adaptive dialogue management in the KRISTINA project.",
                    "En route to a better integration and evaluation of social capacities in vocal artificial agents.",
                    "A corpus for experimental study of affect bursts in human-robot interaction.",
                    "Could a virtual agent be warm and competent? investigating user's impressions of agent's non-verbal behaviours.",
                    "A review of evaluation techniques for social dialogue systems.",
                    "Introducing a ROS based planning and execution framework for human-robot interaction.",
                    "Dialog acts in greeting and leavetaking in social talk.",
                    "Social talk: making conversation with people and machine.",
                    "Analyses of the effects of agents' performing self-adaptors.",
                    "Who has to do it? the use of personal pronouns in human-human and human-robot-interaction.",
                    "Using crowd-sourcing for the design of listening agents: challenges and opportunities.",
                    "Intimately intelligent virtual agents: knowing the human beyond sensory input.",
                    "Integration and evaluation of social competences such as humor in an artificial interactive agent.",
                    "Introducing ADELE: a personalized intelligent companion.",
                    "Recognizing emotions in spoken dialogue with acoustic and lexical cues.",
                    "Multi-modal social interaction recognition using view-invariant features."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2017mie",
                "sub_name": "Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education, MIE@ICMI 2017, Glasgow, United Kingdom, November 13, 2017.",
                "count": 14,
                "papers": [
                    "Developing a pedagogical framework for designing a multisensory serious gaming environment.",
                    "Angle discrimination by walking in children.",
                    "Using force-feedback devices in educational settings: a short review.",
                    "What cognitive and affective states should technology monitor to support learning?",
                    "An unobtrusive and multimodal approach for behavioral engagement detection of students.",
                    "Predicting student engagement in classrooms using facial behavioral cues.",
                    "Evaluation of audio-based feedback technologies for bow learning technique in violin beginners.",
                    "A multimodal LEGO\u00ae-based learning activity mixing musical notation and computer programming.",
                    "An open platform for full-body multisensory serious-games to teach geometry in primary school.",
                    "Automatic generation of actionable feedback towards improving social competency in job interviews.",
                    "Bowing modeling for violin students assistance.",
                    "Air violin: a machine learning approach to fingering gesture recognition.",
                    "A multimodal serious-game to teach fractions in primary school.",
                    "Differences of online learning behaviors and eye-movement between students having different personality traits."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2017mhfi",
                "sub_name": "Proceedings of the 2nd ACM SIGCHI International Workshop on Multisensory Approaches to Human-Food Interaction, MHFI@ICMI 2017, Glasgow, United Kingdom, November 13-17, 2017.",
                "count": 6,
                "papers": [
                    "Are food cinemagraphs more yummy than stills?",
                    "Development of a mobile multi-device nutrition logger.",
                    "Let's drink this song together: interactive taste-sound systems.",
                    "Assessing the impact of music on basic taste perception using time intensity analysis.",
                    "An exploration of taste-emotion mappings from the perspective of food design practitioners.",
                    "Gustatory interface: the challenges of 'how' to stimulate the sense of taste."
                ]
            }
        ]
    },
    {
        "year": "2016",
        "name": "18th ICMI 2016",
        "info": "Tokyo, Japan",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2016",
                "sub_name": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016, Tokyo, Japan, November 12-16, 2016.",
                "count": 112,
                "papers": [
                    "Understanding people by tracking their word use (keynote).",
                    "Learning to generate images and their descriptions (keynote).",
                    "Embodied media: expanding human capacity via virtual reality and telexistence (keynote).",
                    "Help me if you can: towards multiadaptive interaction platforms (ICMI awardee talk).",
                    "Trust me: multimodal signals of trustworthiness.",
                    "Semi-situated learning of verbal and nonverbal content for repeated human-robot interaction.",
                    "Towards building an attentive artificial listener: on the perception of attentiveness in audio-visual feedback tokens.",
                    "Sequence-based multimodal behavior modeling for social agents.",
                    "Adaptive review for mobile MOOC learning via implicit physiological signal sensing.",
                    "Visuotactile integration for depth perception in augmented reality.",
                    "Exploring multimodal biosignal features for stress detection during indoor mobility.",
                    "An IDE for multimodal controls in smart buildings.",
                    "Personalized unknown word detection in non-native language reading using eye gaze.",
                    "Discovering facial expressions for states of amused, persuaded, informed, sentimental and inspired.",
                    "Do speech features for detecting cognitive load depend on specific languages?",
                    "Training on the job: behavioral analysis of job interviews in hospitality.",
                    "Emotion spotting: discovering regions of evidence in audio-visual emotion expressions.",
                    "Semi-supervised model personalization for improved detection of learner's emotional engagement.",
                    "Driving maneuver prediction using car sensor and driver physiological signals.",
                    "On leveraging crowdsourced data for automatic perceived stress detection.",
                    "Investigating the impact of automated transcripts on non-native speakers' listening comprehension.",
                    "Speaker impact on audience comprehension for academic presentations.",
                    "EmoReact: a multimodal approach and dataset for recognizing emotional responses in children.",
                    "Bimanual input for multiscale navigation with pressure and touch gestures.",
                    "Intervention-free selection using EEG and eye tracking.",
                    "Automated scoring of interview videos using Doc2Vec multimodal feature extraction paradigm.",
                    "Estimating communication skills using dialogue acts and nonverbal features in multiple discussion datasets.",
                    "Multi-sensor modeling of teacher instructional segments in live classrooms.",
                    "Meeting extracts for discussion summarization based on multimodal nonverbal information.",
                    "Getting to know you: a multimodal investigation of team behavior and resilience to stress.",
                    "Measuring the impact of multimodal behavioural feedback loops on social interactions.",
                    "Analyzing mouth-opening transition pattern for predicting next speaker in multi-party meetings.",
                    "Automatic recognition of self-reported and perceived emotion: does joint modeling help?",
                    "Personality classification and behaviour interpretation: an approach based on feature categories.",
                    "Multiscale kernel locally penalised discriminant analysis exemplified by emotion recognition in speech.",
                    "Estimating self-assessed personality from body movements and proximity in crowded mingling scenarios.",
                    "Deep learning driven hypergraph representation for image-based emotion recognition.",
                    "Towards a listening agent: a system generating audiovisual laughs and smiles to show interest.",
                    "Sound emblems for affective multimodal output of a robotic tutor: a perception study.",
                    "Automatic detection of very early stage of dementia through multimodal interaction with computer avatars.",
                    "MobileSSI: asynchronous fusion for social signal interpretation in the wild.",
                    "Language proficiency assessment of English L2 speakers based on joint analysis of prosody and native language.",
                    "Training deep networks for facial expression recognition with crowd-sourced label distribution.",
                    "Deep multimodal fusion for persuasiveness prediction.",
                    "Comparison of three implementations of HeadTurn: a multimodal interaction technique with gaze and head turns.",
                    "Effects of multimodal cues on children's perception of uncanniness in a social robot.",
                    "Multimodal feedback for finger-based interaction in mobile augmented reality.",
                    "Smooth eye movement interaction using EOG glasses.",
                    "Active speaker detection with audio-visual co-training.",
                    "Detecting emergent leader in a meeting environment using nonverbal visual features only.",
                    "Stressful first impressions in job interviews.",
                    "Analyzing the articulation features of children's touchscreen gestures.",
                    "Reach out and touch me: effects of four distinct haptic technologies on affective touch in virtual reality.",
                    "Using touchscreen interaction data to predict cognitive workload.",
                    "Exploration of virtual environments on tablet: comparison between tactile and tangible interaction techniques.",
                    "Understanding the impact of personal feedback on face-to-face interactions in the workplace.",
                    "Asynchronous video interviews vs. face-to-face interviews for communication skill measurement: a systematic study.",
                    "Context and cognitive state triggered interventions for mobile MOOC learning.",
                    "Native vs. non-native language fluency implications on multimodal interaction for interpersonal skills training.",
                    "Social signal processing for dummies.",
                    "Metering \"black holes\": networking stand-alone applications for distributed multimodal synchronization.",
                    "Towards a multimodal adaptive lighting system for visually impaired children.",
                    "Multimodal affective feedback: combining thermal, vibrotactile, audio and visual signals.",
                    "Niki and Julie: a robot and virtual human for studying multimodal social interaction.",
                    "A demonstration of multimodal debrief generation for AUVs, post-mission and in-mission.",
                    "Laughter detection in the wild: demonstrating a tool for mobile social signal processing and visualization.",
                    "Multimodal system for public speaking with real time feedback: a positive computing perspective.",
                    "Multimodal biofeedback system integrating low-cost easy sensing devices.",
                    "A telepresence system using a flexible textile display.",
                    "Large-scale multimodal movie dialogue corpus.",
                    "Immersive virtual reality with multimodal interaction and streaming technology.",
                    "Multimodal interaction with the autonomous Android ERICA.",
                    "Ask Alice: an artificial retrieval of information agent.",
                    "Design of multimodal instructional tutoring agents using augmented reality and smart learning objects.",
                    "AttentiveVideo: quantifying emotional responses to mobile video advertisements.",
                    "Young Merlin: an embodied conversational agent in virtual reality.",
                    "EmotiW 2016: video and group-level emotion recognition challenges.",
                    "Emotion recognition in the wild from videos using images.",
                    "A deep look into group happiness prediction from images.",
                    "Video-based emotion recognition using CNN-RNN and C3D hybrid networks.",
                    "LSTM for dynamic emotion and group emotion recognition in the wild.",
                    "Multi-clue fusion for emotion recognition in the wild.",
                    "Multi-view common space learning for emotion recognition in the wild.",
                    "HoloNet: towards robust emotion recognition in the wild.",
                    "Group happiness assessment using geometric features and dataset balancing.",
                    "Happiness level prediction with sequential inputs via multiple regressions.",
                    "Video emotion recognition in the wild based on fusion of multimodal features.",
                    "Wild wild emotion: a multimodal ensemble approach.",
                    "Audio and face video emotion recognition in the wild using deep neural networks and small datasets.",
                    "Automatic emotion recognition in the wild using an ensemble of static and dynamic representations.",
                    "The influence of appearance and interaction strategy of a social robot on the feeling of uncanniness in humans.",
                    "Viewing support system for multi-view videos.",
                    "Engaging children with autism in a shape perception task using a haptic force feedback interface.",
                    "Modeling user's decision process through gaze behavior.",
                    "Multimodal positive computing system for public speaking with real-time feedback.",
                    "Prediction/Assessment of communication skill using multimodal cues in social interactions.",
                    "Player/Avatar body relations in multimodal augmented reality games.",
                    "Computational model for interpersonal attitude expression.",
                    "Assessing symptoms of excessive SNS usage based on user behavior and emotion.",
                    "Kawaii feeling estimation by product attributes and biological signals.",
                    "Multimodal sensing of affect intensity.",
                    "Enriching student learning experience using augmented reality and smart learning objects.",
                    "Automated recognition of facial expressions authenticity.",
                    "Improving the generalizability of emotion recognition systems: towards emotion recognition in the wild.",
                    "Emotion recognition in the wild challenge 2016.",
                    "1st international workshop on embodied interaction with smart environments (workshop summary).",
                    "ASSP4MI2016: 2nd international workshop on advancements in social signal processing for multimodal interaction (workshop summary).",
                    "ERM4CT 2016: 2nd international workshop on emotion representations and modelling for companion systems (workshop summary).",
                    "International workshop on multimodal virtual and augmented reality (workshop summary).",
                    "International workshop on social learning and multimodal interaction for designing artificial agents (workshop summary).",
                    "1st international workshop on multi-sensorial approaches to human-food interaction (workshop summary).",
                    "International workshop on multimodal analyses enabling artificial agents in human- machine interaction (workshop summary)."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016eise",
                "sub_name": "Proceedings of the 1st Workshop on Embodied Interaction with Smart Environments, EISE@ICMI 2016, Tokyo, Japan, November 16, 2016.",
                "count": 5,
                "papers": [
                    "A decoupled three-layered architecture for service robotics in intelligent environments.",
                    "Towards addressee recognition in smart robotic environments: an evidence based approach.",
                    "Challenges for smart environments in bathroom contexts.",
                    "Exploring self-interruptions as a strategy for regaining the attention of distracted users.",
                    "Cross-generational smart environment acceptance: experiences of experiencing smart environments at the LebensPhasenHaus."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016erm4ct",
                "sub_name": "Proceedings of the 2nd workshop on Emotion Representations and Modelling for Companion Systems, ERM4CT@ICMI 2016, Tokyo, Japan, November 16, 2016.",
                "count": 4,
                "papers": [
                    "Shaping the future of education with empathic companions.",
                    "Emotion detection and recognition using HRV features derived from photoplethysmogram signals.",
                    "Inferring mental overload based on postural behavior and gestures.",
                    "Developing recording and synchronous handling of ten modalities in affective human machine interactions."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016mvar",
                "sub_name": "Proceedings of the 2016 workshop on Multimodal Virtual and Augmented Reality, MVAR@ICMI 2016, Tokyo, Japan, November 16, 2016.",
                "count": 9,
                "papers": [
                    "Multimodal augmented reality: the norm rather than the exception.",
                    "(Re-)examination of multimodal augmented reality.",
                    "A networked device for reproducing multisensory kissing.",
                    "Electrical stimulation of olfactory receptors for digitizing smell.",
                    "Implementing new food interactions using magnetic dining table platform and magnetic foods.",
                    "Projective-AR system for customizing the appearance and taste of food.",
                    "Altering resistive force perception by modulating velocity of dot pattern projected onto hand.",
                    "Changing perception of physical properties using multimodal augmented reality: position paper.",
                    "Bringing the augmented reality benefits to biomechanics study."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016ma3hmi",
                "sub_name": "Proceedings of the Workshop on Multimodal Analyses enabling Artificial Agents in Human-Machine Interaction, MA3HMI@ICMI 2016, Tokyo, Japan, November 16, 2016.",
                "count": 10,
                "papers": [
                    "Analysis of gesture frequency and amplitude as a function of personality in virtual agents.",
                    "Deictic gestures in coaching interactions.",
                    "Automatic annotation of gestural units in spontaneous face-to-face interaction.",
                    "Body movements and laughter recognition: experiments in first encounter dialogues.",
                    "Annotation and analysis of listener's engagement based on multi-modal behaviors.",
                    "Assessment of users' interests in multimodal dialog based on exchange unit.",
                    "Attitude recognition of video bloggers using audio-visual descriptors.",
                    "On data driven parametric backchannel synthesis for expressing attentiveness in conversational agents.",
                    "Fitmirror: a smart mirror for positive affect in everyday user morning routines.",
                    "Increasing robustness of multimodal interaction via individual interaction histories."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016mhfi",
                "sub_name": "Proceedings of the 1st Workshop on Multi-sensorial Approaches to Human-Food Interaction, MHFI@ICMI 2016, Tokyo, Japan, November 12 - 16, 2016.",
                "count": 9,
                "papers": [
                    "Multimodal augmented reality: the norm rather than the exception.",
                    "(Re-)examination of multimodal augmented reality.",
                    "A networked device for reproducing multisensory kissing.",
                    "Electrical stimulation of olfactory receptors for digitizing smell.",
                    "Implementing new food interactions using magnetic dining table platform and magnetic foods.",
                    "Projective-AR system for customizing the appearance and taste of food.",
                    "Altering resistive force perception by modulating velocity of dot pattern projected onto hand.",
                    "Changing perception of physical properties using multimodal augmented reality: position paper.",
                    "Bringing the augmented reality benefits to biomechanics study."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016assp4mi",
                "sub_name": "Proceedings of the 2nd Workshop on Advancements in Social Signal Processing for Multimodal Interaction, ASSP4MI@ICMI 2016, Tokyo, Japan, November 12 - 16, 2016.",
                "count": 4,
                "papers": [
                    "Identification of emergent leaders in a meeting scenario using multiple kernel learning.",
                    "Prediction of ice-breaking between participants using prosodic features in the first meeting dialogue.",
                    "Recognizing facial expressions of emotion using action unit specific decision thresholds.",
                    "Look who's talking: visual identification of the active speaker in multi-party human-robot interaction."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2016slmi",
                "sub_name": "Proceedings of the International Workshop on Social Learning and Multimodal Interaction for Designing Artificial Agents, SLMI@ICMI 2016, Tokyo, Japan, November 16, 2016.",
                "count": 5,
                "papers": [
                    "Experimental study to elicit effective multimodal behaviour in pedagogical agents.",
                    "E-VOX: a socially enhanced semantic ECA.",
                    "A multimodal and multilevel system for robotics treatment of autism in children.",
                    "Habit detection within a long-term interaction with a social robot: an exploratory study.",
                    "Scene analysis through auditory event monitoring."
                ]
            }
        ]
    },
    {
        "year": "2015",
        "name": "17th ICMI 2015",
        "info": "Seattle, WA, USA",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2015",
                "sub_name": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 - 13, 2015.",
                "count": 106,
                "papers": [
                    "Sharing Representations for Long Tail Computer Vision Problems.",
                    "Interaction Studies with Social Robots.",
                    "Connections: 2015 ICMI Sustained Accomplishment Award Lecture.",
                    "Combining Two Perspectives on Classifying Multimodal Data for Recognizing Speaker Traits.",
                    "Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery.",
                    "Evaluating Speech, Face, Emotion and Body Movement Time-series Features for Automated Multimodal Presentation Scoring.",
                    "Gender Representation in Cinematic Content: A Multimodal Approach.",
                    "Effects of Good Speaking Techniques on Audience Engagement.",
                    "Multimodal Public Speaking Performance Assessment.",
                    "I Would Hire You in a Minute: Thin Slices of Nonverbal Behavior in Job Interviews.",
                    "Deception Detection using Real-life Trial Data.",
                    "Exploring Turn-taking Cues in Multi-party Human-robot Discussions about Objects.",
                    "Visual Saliency and Crowdsourcing-based Priors for an In-car Situated Dialog System.",
                    "Leveraging Behavioral Patterns of Mobile Applications for Personalized Spoken Language Understanding.",
                    "Who's Speaking?: Audio-Supervised Classification of Active Speakers in Video.",
                    "Predicting Participation Styles using Co-occurrence Patterns of Nonverbal Behaviors in Collaborative Learning.",
                    "Multimodal Fusion using Respiration and Gaze for Predicting Next Speaker in Multi-Party Meetings.",
                    "Deciphering the Silent Participant: On the Use of Audio-Visual Cues for the Classification of Listener Categories in Group Discussions.",
                    "Retrieving Target Gestures Toward Speech Driven Animation with Meaningful Behaviors.",
                    "Look & Pedal: Hands-free Navigation in Zoomable Information Spaces through Gaze-supported Foot Input.",
                    "Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions.",
                    "Digital Flavor: Towards Digitally Simulating Virtual Flavors.",
                    "Different Strokes and Different Folks: Economical Dynamic Surface Sensing and Affect-Related Touch Recognition.",
                    "MPHA: A Personal Hearing Doctor Based on Mobile Devices.",
                    "Towards Attentive, Bi-directional MOOC Learning on Mobile Devices.",
                    "An Experiment on the Feasibility of Spatial Acquisition using a Moving Auditory Cue for Pedestrian Navigation.",
                    "A Wearable Multimodal Interface for Exploring Urban Points of Interest.",
                    "ECA Control using a Single Affective User Dimension.",
                    "Multimodal Interaction with a Bifocal View on Mobile Devices.",
                    "NaLMC: A Database on Non-acted and Acted Emotional Sequences in HCI.",
                    "Exploiting Multimodal Affect and Semantics to Identify Politically Persuasive Web Videos.",
                    "Toward Better Understanding of Engagement in Multiparty Spoken Interaction with Children.",
                    "Gestimator: Shape and Stroke Similarity Based Gesture Recognition.",
                    "Classification of Children's Social Dominance in Group Interactions with Robots.",
                    "Spectators' Synchronization Detection based on Manifold Representation of Physiological Signals: Application to Movie Highlights Detection.",
                    "Implicit User-centric Personality Recognition Based on Physiological Responses to Emotional Videos.",
                    "Detecting Mastication: A Wearable Approach.",
                    "Exploring Behavior Representation for Learning Analytics.",
                    "Multimodal Human Activity Recognition for Industrial Manufacturing Processes in Robotic Workcells.",
                    "Accuracy vs. Availability Heuristic in Multimodal Affect Detection in the Wild.",
                    "Dynamic Active Learning Based on Agreement and Applied to Emotion Recognition in Spoken Interactions.",
                    "Sharing Touch Interfaces: Proximity-Sensitive Touch Targets for Tablet-Mediated Collaboration.",
                    "Analyzing Multimodality of Video for User Engagement Assessment.",
                    "Adjacent Vehicle Collision Warning System using Image Sensor and Inertial Measurement Unit.",
                    "Automatic Detection of Mind Wandering During Reading Using Gaze and Physiology.",
                    "Multimodal Detection of Depression in Clinical Interviews.",
                    "Spoken Interruptions Signal Productive Problem Solving and Domain Expertise in Mathematics.",
                    "Active Haptic Feedback for Touch Enabled TV Remote.",
                    "A Visual Analytics Approach to Finding Factors Improving Automatic Speaker Identifications.",
                    "The Influence of Visual Cues on Passive Tactile Sensations in a Multimodal Immersive Virtual Environment.",
                    "Detection of Deception in the Mafia Party Game.",
                    "Individuality-Preserving Voice Reconstruction for Articulation Disorders Using Text-to-Speech Synthesis.",
                    "Behavioral and Emotional Spoken Cues Related to Mental States in Human-Robot Social Interaction.",
                    "Viewpoint Integration for Hand-Based Recognition of Social Interactions from a First-Person View.",
                    "A Multimodal System for Real-Time Action Instruction in Motor Skill Learning.",
                    "The Application of Word Processor UI paradigms to Audio and Animation Editing.",
                    "CuddleBits: Friendly, Low-cost Furballs that Respond to Touch.",
                    "Public Speaking Training with a Multimodal Interactive Virtual Audience Framework.",
                    "A Multimodal System for Public Speaking with Real Time Feedback.",
                    "Model of Personality-Based, Nonverbal Behavior in Affective Virtual Humanoid Character.",
                    "AttentiveLearner: Adaptive Mobile MOOC Learning via Implicit Cognitive States Inference.",
                    "Interactive Web-based Image Sonification for the Blind.",
                    "Nakama: A Companion for Non-verbal Affective Communication.",
                    "Wir im Kiez: Multimodal App for Mutual Help Among Elderly Neighbours.",
                    "Interact: Tightly-coupling Multimodal Dialog with an Interactive Virtual Assistant.",
                    "The UTEP AGENT System.",
                    "A Distributed Architecture for Interacting with NAO.",
                    "Touch Challenge '15: Recognizing Social Touch Gestures.",
                    "The Grenoble System for the Social Touch Challenge at ICMI 2015.",
                    "Social Touch Gesture Recognition using Random Forest and Boosting on Distinct Feature Sets.",
                    "Recognizing Touch Gestures for Social Human-Robot Interaction.",
                    "Detecting and Identifying Tactile Gestures using Deep Autoencoders, Geometric Moments and Gesture Level Features.",
                    "Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015.",
                    "Hierarchical Committee of Deep CNNs with Exponentially-Weighted Decision Fusion for Static Facial Expression Recognition.",
                    "Image based Static Facial Expression Recognition with Multiple Deep Network Learning.",
                    "Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning.",
                    "Capturing AU-Aware Facial Features and Their Latent Relations for Emotion Recognition in the Wild.",
                    "Contrasting and Combining Least Squares Based Learners for Emotion Recognition in the Wild.",
                    "Recurrent Neural Networks for Emotion Recognition in Video.",
                    "Multiple Models Fusion for Emotion Recognition in the Wild.",
                    "A Deep Feature based Multi-kernel Learning Approach for Video Emotion Recognition.",
                    "Transductive Transfer LDA with Riesz-based Volume LBP for Emotion Recognition in The Wild.",
                    "Combining Multimodal Features within a Fusion Network for Emotion Recognition in the Wild.",
                    "Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns.",
                    "Quantification of Cinematography Semiotics for Video-based Facial Emotion Recognition in the EmotiW 2015 Grand Challenge.",
                    "Affect Recognition using Key Frame Selection based on Minimum Sparse Reconstruction.",
                    "2015 Multimodal Learning and Analytics Grand Challenge.",
                    "Providing Real-time Feedback for Student Teachers in a Virtual Rehearsal Environment.",
                    "Presentation Trainer, your Public Speaking Multimodal Coach.",
                    "Utilizing Depth Sensors for Analyzing Multimodal Presentations: Hardware, Software and Toolkits.",
                    "Multimodal Capture of Teacher-Student Interactions for Automated Dialogic Analysis in Live Classrooms.",
                    "Multimodal Selfies: Designing a Multimodal Recording Device for Students in Traditional Classrooms.",
                    "Temporal Association Rules for Modelling Multimodal Social Signals.",
                    "Detecting and Synthesizing Synchronous Joint Action in Human-Robot Teams.",
                    "Micro-opinion Sentiment Intensity Analysis and Summarization in Online Videos.",
                    "Attention and Engagement Aware Multimodal Conversational Systems.",
                    "Implicit Human-computer Interaction: Two Complementary Approaches.",
                    "Instantaneous and Robust Eye-Activity Based Task Analysis.",
                    "Challenges in Deep Learning for Multimodal Applications.",
                    "Exploring Intent-driven Multimodal Interface for Geographical Information System.",
                    "Software Techniques for Multimodal Input Processing in Realtime Interactive Systems.",
                    "Gait and Postural Sway Analysis, A Multi-Modal System.",
                    "A Computational Model of Culture-Specific Emotion Detection for Artificial Agents in the Learning Domain.",
                    "Record, Transform & Reproduce Social Encounters in Immersive VR: An Iterative Approach.",
                    "Multimodal Affect Detection in the Wild: Accuracy, Availability, and Generalizability.",
                    "Multimodal Assessment of Teaching Behavior in Immersive Rehearsal Environment-TeachLivE."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2015erm4ct",
                "sub_name": "Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies, ERM4CT@ICMI 2015, Seattle, Washington, USA, November 13, 2015.",
                "count": 7,
                "papers": [
                    "ERM4CT 2015: Workshop on Emotion Representations and Modelling for Companion Systems.",
                    "Do's and Don'ts for Software Companions.",
                    "Emotion Unfolding and Affective Scenes: A Case Study in Spoken Conversations.",
                    "Recognising Emotional Evolution from Speech.",
                    "Automatic Assessment of Dimensional Affective Content in Turkish Multi-party Chat Messages.",
                    "Towards a Psycho-Cognitive Recommender System.",
                    "Can a Virtual Listener Replace a Human Listener in Active Listening Conversation?"
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2015interpersonal",
                "sub_name": "Proceedings of the 1st Workshop on Modeling INTERPERsonal SynchrONy And infLuence, INTERPERSONAL@ICMI 2015, Seattle, Washington, USA, November 13, 2015.",
                "count": 9,
                "papers": [
                    "The First International Workshop on Modeling INTERPersonal SynchrONy (INTERPERSONAL 2015).",
                    "Infants' Brains are Wired to Learn from Culture: Implications for Social Robots.",
                    "Exploring Socio-Cognitive Effects of Conversational Strategy Congruence in Peer Tutoring.",
                    "We Click, We Align, We Learn: Impact of Influence and Convergence Processes on Student Learning and Rapport Building.",
                    "Exploring Children's Verbal and Acoustic Synchrony: Towards Promoting Engagement in Speech-Controlled Robot-Companion Games.",
                    "Communicative Behavior and Physiology in Social Interactions.",
                    "Automatic Speaker Identification from Interpersonal Synchrony of Body Motion Behavioral Patterns in Multi-Person Videos.",
                    "Dynamic Time Warping of Multimodal Signals for Detecting Highlights in Movies.",
                    "SyncPy: a Unified Open-source Analytic Library for Synchrony."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2015wmdd",
                "sub_name": "Proceedings of the 2015 ACM Workshop on Multimodal Deception Detection, WMDD@ICMI 2015, Seattle, Washington, USA, November 13, 2015.",
                "count": 5,
                "papers": [
                    "Cross-Cultural Production and Detection of Deception from Speech.",
                    "Trimodal Analysis of Deceptive Behavior.",
                    "Misleading Online Content: Recognizing Clickbait as \"False News\".",
                    "Multimodal Deception Detection: A t-pattern Approach.",
                    "Silesian Deception Database: Presentation and Analysis."
                ]
            }
        ]
    },
    {
        "year": "2014",
        "name": "16th ICMI 2014",
        "info": "Istanbul, Turkey",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2014",
                "sub_name": "Proceedings of the 16th International Conference on Multimodal Interaction, ICMI 2014, Istanbul, Turkey, November 12-16, 2014.",
                "count": 95,
                "papers": [
                    "Bursting our Digital Bubbles: Life Beyond the App.",
                    "Managing Human-Robot Engagement with Forecasts and... um... Hesitations.",
                    "Written Activity, Representations and Fluency as Predictors of Domain Expertise in Mathematics.",
                    "Analysis of Respiration for Prediction of \"Who Will Be Next Speaker and When?\" in Multi-Party Meetings.",
                    "A Multimodal In-Car Dialogue System That Tracks The Driver's Attention.",
                    "Deep Multimodal Fusion: Combining Discrete Events and Continuous Signals.",
                    "The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring.",
                    "Computational Analysis of Persuasiveness in Social Multimedia: A Novel Dataset and Multimodal Prediction Approach.",
                    "Deception detection using a multimodal approach.",
                    "Multimodal Interaction for Future Control Centers: An Interactive Demonstrator.",
                    "Emotional Charades.",
                    "Glass Shooter: Exploring First-Person Shooter Game Control with Google Glass.",
                    "Orchestration for Group Videoconferencing: An Interactive Demonstrator.",
                    "Integrating Remote PPG in Facial Expression Analysis Framework.",
                    "Context-Aware Multimodal Robotic Health Assistant.",
                    "WebSanyog: A Portable Assistive Web Browser for People with Cerebral Palsy.",
                    "The hybrid Agent MARCO.",
                    "Towards Supporting Non-linear Navigation in Educational Videos.",
                    "Detecting conversing groups with a single worn accelerometer.",
                    "Identification of the Driver's Interest Point using a Head Pose Trajectory for Situated Dialog Systems.",
                    "An Explorative Study on Crossmodal Congruence Between Visual and Tactile Icons Based on Emotional Responses.",
                    "Why We Watch the News: A Dataset for Exploring Sentiment in Broadcast Video News.",
                    "Dyadic Behavior Analysis in Depression Severity Assessment Interviews.",
                    "Touching the Void - Introducing CoST: Corpus of Social Touch.",
                    "Unsupervised Domain Adaptation for Personalized Facial Emotion Recognition.",
                    "Predicting Influential Statements in Group Discussions using Speech and Head Motion Information.",
                    "The Relation of Eye Gaze and Face Pose: Potential Impact on Speech Recognition.",
                    "Speech-Driven Animation Constrained by Appropriate Discourse Functions.",
                    "Many Fingers Make Light Work: Non-Visual Capacitive Surface Exploration.",
                    "Multimodal Interaction History and its use in Error Detection and Recovery.",
                    "Gesture Heatmaps: Understanding Gesture Performance with Colorful Visualizations.",
                    "Personal Aesthetics for Soft Biometrics: A Generative Multi-resolution Approach.",
                    "Synchronising Physiological and Behavioural Sensors in a Driving Simulator.",
                    "Data-Driven Model of Nonverbal Behavior for Socially Assistive Human-Robot Interactions.",
                    "Towards Automated Assessment of Public Speaking Skills Using Multimodal Cues.",
                    "Increasing Customers' Attention using Implicit and Explicit Interaction in Urban Advertisement.",
                    "System for Presenting and Creating Smell Effects to Video.",
                    "CrossMotion: Fusing Device and Image Motion for User Identification, Tracking and Device Association.",
                    "Statistical Analysis of Personality and Identity in Chats Using a Keylogging Platform.",
                    "Understanding Users' Perceived Difficulty of Multi-Touch Gesture Articulation.",
                    "A Multimodal Context-based Approach for Distress Assessment.",
                    "Exploring a Model of Gaze for Grounding in Multimodal HRI.",
                    "Predicting Learning and Engagement in Tutorial Dialogue: A Personality-Based Model.",
                    "Eye Gaze for Spoken Language Understanding in Multi-modal Conversational Interactions.",
                    "SoundFLEX: Designing Audio to Guide Interactions with Shape-Retaining Deformable Interfaces.",
                    "Investigating Intrusiveness of Workload Adaptation.",
                    "Smart Multimodal Interaction through Big Data.",
                    "Natural Communication about Uncertainties in Situated Interaction.",
                    "The SWELL Knowledge Work Dataset for Stress and User Modeling Research.",
                    "Rhythmic Body Movements of Laughter.",
                    "Automatic Blinking Detection towards Stress Discovery.",
                    "Mid-air Authentication Gestures: An Exploration of Authentication Based on Palm and Finger Motions.",
                    "Automatic Detection of Naturalistic Hand-over-Face Gesture Descriptors.",
                    "Capturing Upper Body Motion in Conversation: An Appearance Quasi-Invariant Approach.",
                    "User Independent Gaze Estimation by Exploiting Similarity Measures in the Eye Pair Appearance Eigenspace.",
                    "Exploring multimodality for translator-computer interaction.",
                    "Towards Social Touch Intelligence: Developing a Robust System for Automatic Touch Recognition.",
                    "Facial Expression Analysis for Estimating Pain in Clinical Settings.",
                    "Realizing Robust Human-Robot Interaction under Real Environments with Noises.",
                    "Speaker- and Corpus-Independent Methods for Affect Classification in Computational Paralinguistics.",
                    "The Impact of Changing Communication Practices.",
                    "Multi-Resident Human Behaviour Identification in Ambient Assisted Living Environments.",
                    "Gaze-Based Proactive User Interface for Pen-Based Systems.",
                    "Appearance based user-independent gaze estimation.",
                    "Affective Analysis of Abstract Paintings Using Statistical Analysis and Art Theory.",
                    "The Secret Language of Our Body: Affect and Personality Recognition Using Physiological Signals.",
                    "Perceptions of Interpersonal Behavior are Influenced by Gender, Facial Expression Intensity, and Head Pose.",
                    "Authoring Communicative Behaviors for Situated, Embodied Characters.",
                    "Multimodal Analysis and Modeling of Nonverbal Behaviors during Tutoring.",
                    "Computation of Emotions.",
                    "Non-Visual Navigation Using Combined Audio Music and Haptic Cues.",
                    "Tactile Feedback for Above-Device Gesture Interfaces: Adding Touch to Touchless Interactions.",
                    "Once Upon a Crime: Towards Crime Prediction from Demographics and Mobile Data.",
                    "Impact of Coordinate Systems on 3D Manipulations in Mobile Augmented Reality.",
                    "Digital Reading Support for The Blind by Multimodal Interaction.",
                    "Measuring Child Visual Attention using Markerless Head Tracking from Color and Depth Sensing Cameras.",
                    "Bi-Modal Detection of Painful Reaching for Chronic Pain Rehabilitation Systems.",
                    "A World without Barriers: Connecting the World across Languages, Distances and Media.",
                    "Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol.",
                    "Neural Networks for Emotion Recognition in the Wild.",
                    "Emotion Recognition in the Wild: Incorporating Voice and Lip Activity in Multimodal Decision-Level Fusion.",
                    "Combining Multimodal Features with Hierarchical Classifier Fusion for Emotion Recognition in the Wild.",
                    "Combining Modality-Specific Extreme Learning Machines for Emotion Recognition in the Wild.",
                    "Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild.",
                    "Enhanced Autocorrelation in Real World Emotion Recognition.",
                    "Emotion Recognition in the Wild with Feature Fusion and Multiple Kernel Learning.",
                    "Improved Spatiotemporal Local Monogenic Binary Pattern for Emotion Recognition in The Wild.",
                    "Emotion Recognition in Real-world Conditions with Acoustic and Visual Features.",
                    "ERM4HCI 2014: The 2nd Workshop on Emotion Representation and Modelling in Human-Computer-Interaction-Systems.",
                    "Gaze-in 2014: the 7th Workshop on Eye Gaze in Intelligent Human Machine Interaction.",
                    "MAPTRAITS 2014 - The First Audio/Visual Mapping Personality Traits Challenge - An Introduction: Perceived Personality and Social Dimensions.",
                    "MLA'14: Third Multimodal Learning Analytics Workshop and Grand Challenges.",
                    "ICMI 2014 Workshop on Multimodal, Multi-Party, Real-World Human-Robot Interaction.",
                    "An Outline of Opportunities for Multimodal Research.",
                    "UM3I 2014: International Workshop on Understanding and Modeling Multiparty, Multimodal Interactions."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014gazein",
                "sub_name": "Proceedings of the 7th Workshop on Eye Gaze in Intelligent Human Machine Interaction: Eye-Gaze & Multimodality, GazeIn@ICMI 2014, Istanbul, Turkey, November 16, 2014.",
                "count": 8,
                "papers": [
                    "Attention and Gaze in Situated Language Interaction.",
                    "Spatio-Temporal Event Selection in Basic Surveillance Tasks using Eye Tracking and EEG.",
                    "Gaze-Based Virtual Task Predictor.",
                    "Analysis of Timing Structure of Eye Contact in Turn-changing.",
                    "Fusing Multimodal Human Expert Data to Uncover Hidden Semantics.",
                    "Evaluating the Impact of Embodied Conversational Agents (ECAs) Attentional Behaviors on User Retention of Cultural Content in a Simulated Mobile Environment.",
                    "Analyzing Co-occurrence Patterns of Nonverbal Behaviors in Collaborative Learning.",
                    "Study on Participant-controlled Eye Tracker Calibration Procedure."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014erm4hci",
                "sub_name": "Proceedings of the 2014 workshop on Emotion Representation and Modelling in Human-Computer-Interaction-Systems, ERM4HCI@ICMI 2014, Istanbul, Turkey, November 16, 2014.",
                "count": 6,
                "papers": [
                    "An Initial Analysis of Structured Video Interviews by Using Multimodal Emotion Detection.",
                    "A Neural Network Based Approach to Social Touch Classification.",
                    "Emotion Expression and Conversation Assessment in First Acquaintance Dialogues.",
                    "A Design Platform for Emotion-Aware User Interfaces.",
                    "A Model to Incorporate Emotional Sensitivity into Human Computer Interactions.",
                    "Detection of Emotional Events utilizing Support Vector Methods in an Active Learning HCI Scenario."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014rfmir",
                "sub_name": "Proceedings of the 2014 Workshop on Roadmapping the Future of Multimodal Interaction Research including Business Opportunities and Challenges, RFMIR@ICMI 2014, Istanbul, Turkey, November 16, 2014.",
                "count": 13,
                "papers": [
                    "Multimodal Analytics and its Data Ecosystem.",
                    "Topics for the Future: Genre Differentiation, Annotation, and Linguistic Content Integration in Interaction Analysis.",
                    "Role of Inter-Personal Synchrony in Extracting Social Signatures: Some Case Studies.",
                    "Towards Multimodal Pain Assessment for Research and Clinical Use.",
                    "Intra- and Interpersonal Functions of Head Motion in Emotion Communication.",
                    "Statistical Pattern Recognition Meets Formal Ontologies: Towards a Semantic Visual Understanding.",
                    "Cognitive Multimodal Processing: from Signal to Behavior.",
                    "Challenges for Social Embodiment.",
                    "ROCKIT: Roadmap for Conversational Interaction Technologies.",
                    "Natural Multimodal Interaction with a Social Robot: What are the Premises?",
                    "Multimodal Interaction for Future Control Centers: Interaction Concept and Implementation.",
                    "Towards healthcare personal agents.",
                    "Automatic Behaviour Understanding in Medicine."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014mmrwhri",
                "sub_name": "Proceedings of the 2014 Workshop on Multimodal, Multi-Party, Real-World Human-Robot Interaction, Istanbul, Turkey, MMRWHRI@ICMI 2014, November 16, 2014.",
                "count": 10,
                "papers": [
                    "Towards Closed Feedback Loops in HRI: Integrating InproTK and PaMini.",
                    "Attention Detection in Elderly People-Robot Spoken Interaction.",
                    "Advances in Wikipedia-based Interaction with Robots.",
                    "Self-calibration of an Assistive Device to Adapt to Different Users and Environments.",
                    "Towards proactive robot behavior based on incremental language analysis.",
                    "Selection of an Object Requested by Speech Based on Generic Object Recognition.",
                    "Clarification Dialogues for Perception-based Errors in Situated Human-Computer Dialogues.",
                    "Applying Topic Recognition to Spoken Language in Human-Robot Interaction Dialogues.",
                    "Applying Semantic Web Services to Multi-Robot Coordination.",
                    "Affective Feedback for a Virtual Robot in a Real-World Treasure Hunt."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014um3i",
                "sub_name": "Proceedings of the 2014 Workshop on Understanding and Modeling Multiparty, Multimodal Interactions, UM3I@ICMI 2014, Istanbul, Turkey, November 16, 2014.",
                "count": 9,
                "papers": [
                    "From Modeling Multimodal and Multiparty Interactions to Designing Conversational Agents.",
                    "Context in Affective Multiparty and Multimodal Interaction: Why, Which, How and Where?",
                    "Effect of nonverbal behavioral patterns on the performance of small groups.",
                    "Robo fashion world: a multimodal corpus of multi-child human-computer interaction.",
                    "Comparison of Human-Human and Human-Robot Turn-Taking Behaviour in Multiparty Situated Interaction.",
                    "Who Will Get the Grant?: A Multimodal Corpus for the Analysis of Conversational Behaviours in Group Interviews.",
                    "Eye Gaze Analyses in L1 and L2 Conversations: From the Perspective of Listeners' Eye Gaze Activity.",
                    "Mitigating problems in video-mediated group discussions: Towards conversation aware video-conferencing systems.",
                    "Models for Decision Making in Video Mediated Communication."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014maptraits",
                "sub_name": "Proceedings of the 2014 Workshop on Mapping Personality Traits Challenge and Workshop, MAPTRAITS@ICMI 2014, Istanbul, Turkey, November 12, 2014.",
                "count": 5,
                "papers": [
                    "Personality Computing: How Machines Can Deal With Personality Traits.",
                    "MAPTRAITS 2014: The First Audio/Visual Mapping Personality Traits Challenge.",
                    "Automatic Recognition of Personality Traits: A Multimodal Approach.",
                    "Continuous Mapping of Personality Traits: A Novel Challenge and Failure Conditions.",
                    "Acoustic Gait-based Person Identification using Hidden Markov Models."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2014mla",
                "sub_name": "Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge, MLA@ICMI 2014, Istanbul, Turkey, November 12, 2014.",
                "count": 8,
                "papers": [
                    "Multimodal Learning Analytics as a Tool for Bridging Learning Theory and Complex Learning Behaviors.",
                    "Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues.",
                    "Holistic Analysis of the Classroom.",
                    "Deciphering the Practices and Affordances of Different Reasoning Strategies through Multimodal Learning Analytics.",
                    "Combining empirical and machine learning techniques to predict math expertise using pen signal features.",
                    "Estimation of Presentations Skills Based on Slides and Audio Features.",
                    "Using Multimodal Cues to Analyze MLA'14 Oral Presentation Quality Corpus: Presentation Delivery and Slides Quality.",
                    "Presentation Skills Estimation Based on Video and Kinect Data Analysis."
                ]
            }
        ]
    },
    {
        "year": "2013",
        "name": "15th ICMI 2013",
        "info": "Sydney, NSW, Australia",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2013",
                "sub_name": "2013 International Conference on Multimodal Interaction, ICMI '13, Sydney, NSW, Australia, December 9-13, 2013.",
                "count": 100,
                "papers": [
                    "Behavior imaging and the study of autism.",
                    "On the relationship between head pose, social attention and personality prediction for unstructured and dynamic group interactions.",
                    "One of a kind: inferring personality impressions in meetings.",
                    "Who is persuasive?: the role of perceived personality and communication modality in social multimedia.",
                    "Going beyond traits: multimodal classification of personality states in the wild.",
                    "Implementation and evaluation of a multimodal addressee identification mechanism for multiparty conversation systems.",
                    "Managing chaos: models of turn-taking in character-multichild interactions.",
                    "Speaker-adaptive multimodal prediction model for listener responses.",
                    "User experiences of mobile audio conferencing with spatial audio, haptics and gestures.",
                    "A framework for multimodal data collection, visualization, annotation and learning.",
                    "Demonstration of sketch-thru-plan: a multimodal interface for command and control.",
                    "Robotic learning companions for early language development.",
                    "WikiTalk human-robot interactions.",
                    "Saliency-guided 3D head pose estimation on 3D expression models.",
                    "Predicting next speaker and timing from gaze transition patterns in multi-party meetings.",
                    "A semi-automated system for accurate gaze coding in natural dyadic interactions.",
                    "Evaluating the robustness of an appearance-based gaze estimation method for multimodal interfaces.",
                    "A gaze-based method for relating group involvement to individual engagement in multimodal multiparty dialogue.",
                    "Leveraging the robot dialog state for visual focus of attention recognition.",
                    "CoWME: a general framework to evaluate cognitive workload during multimodal interaction.",
                    "Hi YouTube!: personality impressions and verbal content in social video.",
                    "Cross-domain personality prediction: from video blogs to small group meetings.",
                    "Automatic detection of deceit in verbal communication.",
                    "Audiovisual behavior descriptors for depression assessment.",
                    "A Markov logic framework for recognizing complex events from multimodal data.",
                    "Interactive relevance search and modeling: support for expert-driven analysis of multimodal data.",
                    "Predicting speech overlaps from speech tokens and co-occurring body behaviours in dyadic conversations.",
                    "Interaction analysis and joint attention tracking in augmented reality.",
                    "Mo!Games: evaluating mobile gestures in the wild.",
                    "Timing and entrainment of multimodal backchanneling behavior for an embodied conversational agent.",
                    "Video analysis of approach-avoidance behaviors of teenagers speaking with virtual agents.",
                    "A dialogue system for multimodal human-robot interaction.",
                    "The zigzag paradigm: a new P300-based brain computer interface.",
                    "SpeeG2: a speech- and gesture-based interface for efficient controller-free text input.",
                    "Interfaces for thinkers: computer input capabilities that support inferential reasoning.",
                    "Adaptive timeline interface to personal history data.",
                    "Learning a sparse codebook of facial and body microexpressions for emotion recognition.",
                    "Giving interaction a hand: deep models of co-speech gesture in multimodal systems.",
                    "Five key challenges in end-user development for tangible and embodied interaction.",
                    "How can i help you': comparing engagement classification strategies for a robot bartender.",
                    "Comparing task-based and socially intelligent behaviour in a robot bartender.",
                    "A dynamic multimodal approach for assessing learners' interaction experience.",
                    "Relative accuracy measures for stroke gestures.",
                    "LensGesture: augmenting mobile interactions with back-of-device finger gestures.",
                    "Aiding human discovery of handwriting recognition errors.",
                    "Context-based conversational hand gesture classification in narrative interaction.",
                    "A haptic touchscreen interface for mobile devices.",
                    "A social interaction system for studying humor with the Robot NAO.",
                    "TaSST: affective mediated touch.",
                    "Talk ROILA to your Robot.",
                    "NEMOHIFI: an affective HiFi agent.",
                    "Persuasiveness in social multimedia: the role of communication modality and the challenge of crowdsourcing annotations.",
                    "Towards a dynamic view of personality: multimodal classification of personality states in everyday situations.",
                    "Designing effective multimodal behaviors for robots: a data-driven perspective.",
                    "Controllable models of gaze behavior for virtual agents and humanlike robots.",
                    "The nature of the bots: how people respond to robots, virtual agents and humans as multimodal stimuli.",
                    "Adaptive virtual rapport for embodied conversational agents.",
                    "3D head pose and gaze tracking and their application to diverse multimodal tasks.",
                    "Towards developing a model for group involvement and individual engagement.",
                    "Gesture recognition using depth images.",
                    "Modeling semantic aspects of gaze behavior while catalog browsing.",
                    "Computational behaviour modelling for autism diagnosis.",
                    "ChaLearn multi-modal gesture recognition 2013: grand challenge and workshop summary.",
                    "Emotion recognition in the wild challenge (EmotiW) challenge and workshop summary.",
                    "ICMI 2013 grand challenge workshop on multimodal learning analytics.",
                    "Hands and speech in space: multimodal interaction with augmented reality interfaces.",
                    "Evaluating dual-view perceptual issues in handheld augmented reality: device vs. user perspective rendering.",
                    "MM+Space: n x 4 degree-of-freedom kinetic display for recreating multiparty conversation spaces.",
                    "Investigating appropriate spatial relationship between user and ar character agent for communication using AR WoZ system.",
                    "Inferring social activities with mobile sensor networks.",
                    "Effects of language proficiency on eye-gaze in second language conversations: toward supporting second language collaboration.",
                    "Predicting where we look from spatiotemporal gaps.",
                    "Automatic multimodal descriptors of rhythmic body movement.",
                    "Multimodal analysis of body communication cues in employment interviews.",
                    "Multi-modal gesture recognition challenge 2013: dataset and results.",
                    "Fusing multi-modal features for gesture recognition.",
                    "A multi modal approach to gesture recognition from audio and video data.",
                    "Online RGB-D gesture recognition with extreme learning machines.",
                    "A multi-modal gesture recognition system using audio, video, and skeletal joint data.",
                    "ChAirGest: a challenge for multimodal mid-air gesture recognition for close HCI.",
                    "Gesture spotting and recognition using salience detection and concatenated hidden markov models.",
                    "Multi-modal social signal analysis for predicting agreement in conversation settings.",
                    "Multi-modal descriptors for multi-class hand pose recognition in human computer interaction systems.",
                    "Emotion recognition in the wild challenge 2013.",
                    "Multiple kernel learning for emotion recognition in the wild.",
                    "Partial least squares regression on grassmannian manifold for emotion recognition.",
                    "Emotion recognition with boosted tree classifiers.",
                    "Distribution-based iterative pairwise classification of emotions in the wild using LGBP-TOP.",
                    "Combining modality specific deep neural networks for emotion recognition in video.",
                    "Multi classifier systems and forward backward feature selection algorithms to classify emotional coloured speech.",
                    "Emotion recognition using facial and audio features.",
                    "Multimodal learning analytics: description of math data corpus for ICMI grand challenge workshop.",
                    "Problem solving, domain expertise and learning: ground-truth performance results for math data corpus.",
                    "Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction.",
                    "Expertise estimation based on simple multimodal features.",
                    "Using micro-patterns of speech to predict the correctness of answers to mathematics problems: an exercise in multimodal learning analytics.",
                    "Written and multimodal representations as predictors of expertise and problem-solving success in mathematics.",
                    "ERM4HCI 2013: the 1st workshop on emotion representation and modelling in human-computer-interaction-systems.",
                    "Gazein'13: the 6th workshop on eye gaze in intelligent human machine interaction: gaze in multimodal interaction.",
                    "Smart material interfaces: \"another step to a material future\"."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2013emotiw",
                "sub_name": "Proceedings of the 2013 on Emotion recognition in the wild challenge and workshop, EmotiW 2013, Sydney, Australia, December 9, 2013.",
                "count": 3,
                "papers": [
                    "Facing reality: an industrial view on large scale use of facial expression analysis.",
                    "Why is facial expression analysis in the wild challenging?",
                    "Evaluation of vision-based real-time measures for emotions discrimination under uncontrolled conditions."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2013gazein",
                "sub_name": "Proceedings of the 6th workshop on Eye gaze in intelligent human machine interaction: gaze in multimodal interaction, GazeIn@ICMI 2013, Sydney, Australia, December 13, 2013.",
                "count": 11,
                "papers": [
                    "Context aware addressee estimation for human robot interaction.",
                    "The acoustics of eye contact: detecting visual attention from conversational audio cues.",
                    "A dominance estimation mechanism using eye-gaze and turn-taking information.",
                    "Finding the timings for a guide agent to interveneinter-user conversation in considering their gazebehaviors.",
                    "Situated multi-modal dialog system in vehicles.",
                    "Agent-assisted multi-viewpoint video viewer and its gaze-based evaluation.",
                    "Mutual disambiguation of eye gaze and speech for sight translation and reading.",
                    "Learning aspects of interest from Gaze.",
                    "Feature selection for gaze, pupillary, and EEG signals evoked in a 3D environment.",
                    "Lying through the eyes: detecting lies through eye movements.",
                    "Unrawelling the interaction strategies and gaze in collaborative learning with online video lectures."
                ]
            },
            {
                "sub_name_abbr": "conf/icmi/2013smi",
                "sub_name": "Proceedings of the second international workshop on Smart material interfaces: another step to a material future, SMI 2013, Sydney, Australia, December 13, 2013.",
                "count": 5,
                "papers": [
                    "Smart material interfaces as a methodology for interaction: a survey of SMIs' state of the art and development.",
                    "Electronic origami with the color-changing function.",
                    "UISilk: towards interfacing the body.",
                    "Using ForceForm, a dynamically deformable interactive surface, for palpation simulation in medical scenarios.",
                    "An interface composed of a collection of \"smart hairs\"."
                ]
            }
        ]
    },
    {
        "year": "2012",
        "name": "14th ICMI 2012",
        "info": "Santa Monica, CA, USA",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2012",
                "sub_name": "International Conference on Multimodal Interaction, ICMI '12, Santa Monica, CA, USA, October 22-26, 2012.",
                "count": 111,
                "papers": [
                    "The co-operative, transformative organization of human action and knowledge.",
                    "Two people walk into a bar: dynamic multi-party social interaction with a robot agent.",
                    "Changes in verbal and nonverbal conversational behavior in long-term interaction.",
                    "I already know your answer: using nonverbal behaviors to predict immediate outcomes in a dyadic negotiation.",
                    "Modeling dominance effects on nonverbal behaviors using granger causality.",
                    "Multimodal human behavior analysis: learning correlation and interaction across modalities.",
                    "Consistent but modest: a meta-analysis on unimodal and multimodal affect detection accuracies from 30 studies.",
                    "Multimodal recognition of personality traits in human-computer collaborative tasks.",
                    "Automatic detection of pain intensity.",
                    "FaceTube: predicting personality from facial expressions of emotion in online conversational video.",
                    "The blue one to the left: enabling expressive user interaction in a multimodal interface for object selection in virtual 3d environments.",
                    "Pixene: creating memories while sharing photos.",
                    "Designing multiuser multimodal gestural interactions for the living room.",
                    "Using explanations for runtime dialogue adaptation.",
                    "NeuroDialog: an EEG-enabled spoken dialog interface.",
                    "Companion technology for multimodal interaction.",
                    "IrisTK: a statechart-based toolkit for multi-party face-to-face interaction.",
                    "Estimating conversational dominance in multiparty interaction.",
                    "Learning relevance from natural eye movements in pervasive interfaces.",
                    "Fishing or a Z?: investigating the effects of error on mimetic and alphabet device-based gesture interaction.",
                    "Structural and temporal inference search (STIS): pattern identification in multimodal data.",
                    "Integrating word acquisition and referential grounding towards physical world interaction.",
                    "Effects of modality on virtual button motion and performance.",
                    "Modeling multimodal integration with event logic charts.",
                    "Multimodal motion guidance: techniques for adaptive and dynamic feedback.",
                    "Multimodal detection of salient behaviors of approach-avoidance in dyadic interactions.",
                    "Multimodal analysis of the implicit affective channel in computer-mediated textual communication.",
                    "Towards sensing the influence of visual narratives on human affect.",
                    "Integrating video and accelerometer signals for nocturnal epileptic seizure detection.",
                    "GeoGazemarks: providing gaze history for the orientation on small display maps.",
                    "Lost in navigation: evaluating a mobile map app for a fair.",
                    "An evaluation of game controllers and tablets as controllers for interactive tv applications.",
                    "Towards multimodal deception detection - step 1: building a collection of deceptive videos.",
                    "A portable audio/video recorder for longitudinal study of child development.",
                    "Integrating PAMOCAT in the research cycle: linking motion capturing and conversation analysis.",
                    "Motion retrieval based on kinetic features in large motion database.",
                    "Vision-based handwriting recognition for unrestricted text input in mid-air.",
                    "Investigating the midline effect for visual focus of attention recognition.",
                    "Let's have dinner together: evaluate the mediated co-dining experience.",
                    "Infusing the physical world into user interfaces.",
                    "Child-computer interaction: ICMI 2012 special session.",
                    "Knowledge gaps in hands-on tangible interaction research.",
                    "Evaluating artefacts with children: age and technology effects in the reporting of expected and experienced fun.",
                    "Measuring enjoyment of an interactive museum experience.",
                    "Bifocal modeling: a study on the learning outcomes of comparing physical and computational models linked in real time.",
                    "Connecting play: understanding multimodal participation in virtual worlds.",
                    "Gestures as point clouds: a $P recognizer for user interface prototypes.",
                    "Influencing gestural representation of eventualities: insights from ontology.",
                    "Using self-context for multimodal detection of head nods in face-to-face interactions.",
                    "Multimodal multiparty social interaction with the furhat head.",
                    "An avatar-based help system for a grid computing web portal.",
                    "GamEMO: how physiological signals show your emotions and enhance your game experience.",
                    "Multimodal collaboration for crime scene investigation in mediated reality.",
                    "PAMOCAT: linking motion capturing and conversation analysis.",
                    "Multimodal dialogue in mobile local search.",
                    "Toward an argumentation-based dialogue framework for human-robot collaboration.",
                    "Timing multimodal turn-taking for human-robot cooperation.",
                    "My automated conversation helper (MACH): helping people improve social skills.",
                    "A touch of affect: mediated social touch and affect.",
                    "Depression analysis: a multimodal approach.",
                    "Design space for finger gestures with hand-held tablets.",
                    "Multi-modal interfaces for control of assistive robotic devices.",
                    "Space, speech, and gesture in human-robot interaction.",
                    "Machine analysis and recognition of social contexts.",
                    "Task-learning policies for collaborative task solving in human-robot interaction.",
                    "Simulating real danger?: validation of driving simulator test and psychological factors in brake response time to danger.",
                    "Virtual patients to teach cultural competency.",
                    "Multimodal learning analytics: enabling the future of learning through multimodal data analysis and interfaces.",
                    "A hierarchical approach to continuous gesture analysis for natural multi-modal interaction.",
                    "AVEC 2012: the continuous audio/visual emotion challenge - an introduction.",
                    "ICMI'12 grand challenge: haptic voice recognition.",
                    "Audio-visual robot command recognition: D-META'12 grand challenge.",
                    "Brain computer interfaces as intelligent sensors for enhancing human-computer interaction.",
                    "Using psychophysical techniques to design and evaluate multimodal interfaces: psychophysics and interface design.",
                    "Reproducing materials of virtual elements on touchscreens using supplemental thermal feedback.",
                    "Feeling it: the roles of stiffness, deformation range and feedback in the control of deformable ui.",
                    "Audible rendering of text documents controlled by multi-touch interaction.",
                    "Taste/IP: the sensation of taste for digital communication.",
                    "Learning speaker, addressee and overlap detection models from multimodal streams.",
                    "Analysis of the correlation between the regularity of work behavior and stress indices based on longitudinal behavioral data.",
                    "Linking speaking and looking behavior patterns with group composition, perception, and performance.",
                    "Semi-automatic generation of multimodal user interfaces for dialogue-based interactive systems.",
                    "Designing multimodal reminders for the home: pairing content with presentation.",
                    "AVEC 2012: the continuous audio/visual emotion challenge.",
                    "Facial emotion recognition with expression energy.",
                    "Multiple classifier combination using reject options and markov fusion networks.",
                    "Audio-visual emotion challenge 2012: a simple approach.",
                    "Step-wise emotion recognition using concatenated-HMM.",
                    "Combining video, audio and lexical indicators of affect in spontaneous conversation via particle filtering.",
                    "A multimodal fuzzy inference system using a continuous facial expression representation for emotion detection.",
                    "Robust continuous prediction of human emotions using multiscale dynamic cues.",
                    "Elastic net for paralinguistic speech recognition.",
                    "Improving generalisation and robustness of acoustic affect recognition.",
                    "Preserving actual dynamic trend of emotion in dimensional speech emotion recognition.",
                    "Negative sentiment in scenarios elicit pupil dilation response: an auditory study.",
                    "Design and implementation of the note-taking style haptic voice recognition for mobile devices.",
                    "Development of the 2012 SJTU HVR system.",
                    "Improving mandarin predictive text input by augmenting pinyin initials with speech and tonal information.",
                    "LUI: lip in multimodal mobile GUI interaction.",
                    "Speak-as-you-swipe (SAYS): a multimodal interface combining speech and gesture keyboard synchronously for continuous mobile text entry.",
                    "Interpersonal biocybernetics: connecting through social psychophysiology.",
                    "Adaptive EEG artifact rejection for cognitive games.",
                    "Construction of the biocybernetic loop: a case study.",
                    "An interactive control strategy is more robust to non-optimal classification boundaries.",
                    "Improving BCI performance after classification.",
                    "Electroencephalographic detection of visual saliency of motion towards a practical brain-computer interface for video analysis.",
                    "Workshop on speech and gesture production in virtually and physically embodied conversational agents.",
                    "1st international workshop on multimodal learning analytics: extended abstract.",
                    "4th workshop on eye gaze in intelligent human machine interaction: eye gaze and multimodality.",
                    "The 3rd international workshop on social behaviour in music: SBM2012.",
                    "Smart material interfaces: a material step to the future."
                ]
            }
        ]
    },
    {
        "year": "2011",
        "name": "13th ICMI 2011",
        "info": "Alicante, Spain",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2011",
                "sub_name": "Proceedings of the 13th International Conference on Multimodal Interfaces, ICMI 2011, Alicante, Spain, November 14-18, 2011.",
                "count": 65,
                "papers": [
                    "Still looking at people.",
                    "Mining multimodal sequential patterns: a case study on affect detection.",
                    "Crowdsourced data collection of facial responses.",
                    "A systematic discussion of fusion techniques for multi-modal affect recognition tasks.",
                    "Adaptive facial expression recognition using inter-modal top-down context.",
                    "Brain-computer interaction: can multimodality help?",
                    "Modality switching and performance in a thought and speech controlled computer game.",
                    "An approach towards human-robot-human interaction using a hybrid brain-computer interface.",
                    "Towards multimodal error responses: a passive BCI for the detection of auditory errors.",
                    "Pseudo-haptics: from the theoretical foundations to practical system design guidelines.",
                    "6th senses for everyone!: the value of multimodal feedback in handheld navigation aids.",
                    "Adding haptic feedback to touch screens at the right time.",
                    "Robust user context analysis for multimodal interfaces.",
                    "The picture says it all!: multimodal interactions and interaction metadata.",
                    "Mudra: a unified multimodal interaction framework.",
                    "Humans and smart environments: a novel multimodal interaction approach.",
                    "Exploiting petri-net structure for activity classification and user instruction within an industrial setting.",
                    "JerkTilts: using accelerometers for eight-choice selection on mobile devices.",
                    "On multimodal interactive machine translation using speech recognition.",
                    "Multimodal segmentation of object manipulation sequences with product models.",
                    "Could a dialog save your life?: analyzing the effects of speech interaction strategies while driving.",
                    "Decisions about turns in multiparty conversation: from perception to action.",
                    "Evaluation of user gestures in multi-touch interaction: a case study in pair-programming.",
                    "Towards multimodal sentiment analysis: harvesting opinions from the web.",
                    "The impact of unwanted multimodal notifications.",
                    "Freeform pen-input as evidence of cognitive load and expertise.",
                    "Acquisition of dynamically revealed multimodal targets.",
                    "Emotional responses to thermal stimuli.",
                    "An active learning scenario for interactive machine translation.",
                    "Move, and i will tell you who you are: detecting deceptive roles in low-quality data.",
                    "Multimodal person independent recognition of workload related biosignal patterns.",
                    "Study of different interactive editing operations in an assisted transcription system.",
                    "Dynamic perception-production oscillation model in human-machine communication.",
                    "The effect of clothing on thermal feedback perception.",
                    "Comparing multi-touch interaction techniques for manipulation of an abstract parameter space.",
                    "A general framework for incremental processing of multimodal inputs.",
                    "Learning in and from humans: recalibration makes (the) perfect sense.",
                    "Detecting F-formations as dominant sets.",
                    "Toward multimodal situated analysis.",
                    "Finding audio-visual events in informal social gatherings.",
                    "Please, tell me about yourself: automatic personality assessment using short self-presentations.",
                    "Gesture-aware remote controls: guidelines and interaction technique.",
                    "The effect of sampling rate on the performance of template-based gesture recognizers.",
                    "American sign language recognition with the kinect.",
                    "Perceived physicality in audio-enhanced force input.",
                    "BeeParking: an ambient display to induce cooperative parking behavior.",
                    "Speech interaction in a multimodal tool for handwritten text transcription.",
                    "Digital pen in mammography patient forms.",
                    "MozArt: a multimodal interface for conceptual 3D modeling.",
                    "Query refinement suggestion in multimodal image retrieval with relevance feedback.",
                    "A multimodal music transcription prototype: first steps in an interactive prototype development.",
                    "Socially assisted multi-view video viewer.",
                    "Long-term socially perceptive and interactive robot companions: challenges and future perspectives.",
                    "Living with a robot companion: empirical study on the interaction with an artificial health advisor.",
                    "Child-robot interaction in the wild: advice to the aspiring experimenter.",
                    "Characterization of coordination in an imitation task: human evaluation and automatically computable cues.",
                    "The sounds of social life: observing humans in their natural habitat.",
                    "Smartphone usage in the wild: a large-scale analysis of applications and context.",
                    "Multimodal mobile interactions: usability studies in real world settings.",
                    "Service-oriented autonomic multimodal interaction in a pervasive environment.",
                    "Evaluation of graphical user-interfaces for order picking using head-mounted displays.",
                    "Modeling parallel state charts for multithreaded multimodal dialogues.",
                    "Virtual worlds and active learning for human detection.",
                    "Making virtual conversational agent aware of the addressee of users' utterances in multi-user conversation using nonverbal information.",
                    "Temporal binding of multimodal controls for dynamic map displays: a systems approach."
                ]
            }
        ]
    },
    {
        "year": "2010",
        "name": "12th ICMI / 7. MLMI 2010",
        "info": "Beijing, China",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2010",
                "sub_name": "Proceedings of the 12th International Conference on Multimodal Interfaces / 7. International Workshop on Machine Learning for Multimodal Interaction, ICMI-MLMI 2010, Beijing, China, November 8-12, 2010.",
                "count": 54,
                "papers": [
                    "Language and thought: talking, gesturing (and signing) about space.",
                    "Feedback is... late: measuring multimodal delays in mobile device touchscreen interaction.",
                    "Learning and evaluating response prediction models using parallel listener consensus.",
                    "Real-time adaptive behaviors in multimodal human-avatar interactions.",
                    "Facilitating multiparty dialog with gaze, gesture, and speech.",
                    "Focusing computational visual attention in multi-modal human-robot interaction.",
                    "Employing social gaze and speaking activity for automatic determination of the Extraversion trait.",
                    "Gaze quality assisted automatic recognition of social contexts in collaborative Tetris.",
                    "Discovering eye gaze behavior during human-agent conversation in an interactive storytelling application.",
                    "Speak4it: multimodal interaction for local search.",
                    "A multimodal interactive text generation system.",
                    "The Ambient Spotlight: personal multimodal search without query.",
                    "Cloud mouse: a new way to interact with the cloud.",
                    "Musical performance as multimodal communication: drummers, musical collaborators, and listeners.",
                    "Toward natural interaction in the real world: real-time gesture recognition.",
                    "Gesture and voice prototyping for early evaluations of social acceptability in multimodal interfaces.",
                    "Automatic recognition of sign language subwords based on portable accelerometer and EMG sensors.",
                    "Enabling multimodal discourse for the blind.",
                    "Recommendation from robots in a real-world retail shop.",
                    "Dynamic user interface distribution for flexible multimodal interaction.",
                    "3D-press: haptic illusion of compliance when pressing on a rigid surface.",
                    "Understanding contextual factors in location-aware multimedia messaging.",
                    "Embedded media barcode links: optimally blended barcode overlay on paper for linking to associated media.",
                    "Enhancing browsing experience of table and image elements in web pages.",
                    "PhotoMagnets: supporting flexible browsing and searching in photo collections.",
                    "A language-based approach to indexing heterogeneous multimedia lifelog.",
                    "Human-centered attention models for video summarization.",
                    "Activity-based Ubicomp: a new research basis for the future of human-computer interaction.",
                    "Visual speech synthesis by modelling coarticulation dynamics using a non-parametric switching state-space model.",
                    "Multi-modal computer assisted speech transcription.",
                    "Grounding spatial language for video search.",
                    "Location grounding in multimodal local search.",
                    "Linearity and synchrony: quantitative metrics for slide-based presentation methodology.",
                    "Empathetic video experience through timely multimodal interaction.",
                    "Haptic numbers: three haptic representation models for numbers on a touch screen phone.",
                    "Key-press gestures recognition and interaction based on SEMG signals.",
                    "Mood avatar: automatic text-driven head motion synthesis.",
                    "Does haptic feedback change the way we view touchscreens in cars?",
                    "Identifying emergent leadership in small groups using nonverbal communicative cues.",
                    "Quantifying group problem solving with stochastic analysis.",
                    "Cognitive skills learning: pen input patterns in computer-based athlete training.",
                    "Vocal sketching: a prototype tool for designing multimodal interaction.",
                    "Evidence-based automated traffic hazard zone mapping using wearable sensors.",
                    "Analysis environment of conversational structure with nonverbal multimodal data.",
                    "Design and evaluation of a wearable remote social touch device.",
                    "Multimodal interactive machine translation.",
                    "Component-based high fidelity interactive prototyping of post-WIMP interactions.",
                    "Active learning strategies for handwritten text transcription.",
                    "Behavior and preference in minimal personality: a study on embodied conversational agents.",
                    "Vlogcast yourself: nonverbal behavior and attention in social media.",
                    "3D user-perspective, voxel-based estimation of visual focus of attention in dynamic meeting scenarios.",
                    "Modelling and analyzing multimodal dyadic interactions using social networks.",
                    "Analyzing multimodal time series as dynamical systems.",
                    "Conversation scene analysis based on dynamic Bayesian network and image-based gaze detection."
                ]
            }
        ]
    },
    {
        "year": "2009",
        "name": "11th ICMI 2009",
        "info": "Cambridge, Massachusetts, USA",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2009",
                "sub_name": "Proceedings of the 11th International Conference on Multimodal Interfaces, ICMI 2009, Cambridge, Massachusetts, USA, November 2-4, 2009.",
                "count": 60,
                "papers": [
                    "Living better with robots.",
                    "Discovering group nonverbal conversational patterns with topics.",
                    "Agreement detection in multiparty conversation.",
                    "Multimodal floor control shift detection.",
                    "Static vs. dynamic modeling of human nonverbal behavior from multiple cues and modalities.",
                    "Dialog in the open world: platform and applications.",
                    "Towards adapting fantasy, curiosity and challenge in multimodal dialogue systems for preschoolers.",
                    "Building multimodal applications with EMMA.",
                    "A speaker diarization method based on the probabilistic fusion of audio-visual location information.",
                    "Dynamic robot autonomy: investigating the effects of robot decision-making in a human-robot team task.",
                    "A speech mashup framework for multimodal mobile services.",
                    "Detecting, tracking and interacting with people in a public space.",
                    "Cache-based language model adaptation using visual attention for ASR in meeting scenarios.",
                    "Multimodal end-of-turn prediction in multi-party meetings.",
                    "Recognizing communicative facial expressions for discovering interpersonal emotions in group meetings.",
                    "Classification of patient case discussions through analysis of vocalisation graphs.",
                    "Learning from preferences and selected multimodal features of players.",
                    "Detecting user engagement with a robot companion using task and social interaction-based features.",
                    "Multi-modal features for real-time detection of human-robot interaction categories.",
                    "Modeling culturally authentic style shifting with virtual peers.",
                    "Between linguistic attention and gaze fixations inmultimodal conversational interfaces.",
                    "Head-up interaction: can we break our addiction to the screen and keyboard?",
                    "Fusion engines for multimodal input: a survey.",
                    "A fusion framework for multimodal interactive applications.",
                    "Benchmarking fusion engines of multimodal interactive systems.",
                    "Temporal aspects of CARE-based multimodal fusion: from a fusion mechanism to composition components and WoZ components.",
                    "Formal description techniques to support the design, construction and evaluation of fusion engines for sure (safe, usable, reliable and evolvable) multimodal interfaces.",
                    "Multimodal inference for driver-vehicle interaction.",
                    "Multimodal integration of natural gaze behavior for intention recognition during object manipulation.",
                    "Salience in the generation of multimodal referring acts.",
                    "Communicative gestures in coreference identification in multiparty meetings.",
                    "Realtime meeting analysis and 3D meeting viewer based on omnidirectional multimodal sensors.",
                    "Guiding hand: a teaching tool for handwriting.",
                    "A multimedia retrieval system using speech input.",
                    "Navigation with a passive brain based interface.",
                    "A multimodal predictive-interactive application for computer assisted transcription and translation.",
                    "Multi-modal communication system.",
                    "HephaisTK: a toolkit for rapid prototyping of multimodal interfaces.",
                    "State, : an assisted document transcription system.",
                    "Demonstration: first steps in emotional expression of the humanoid robot Nao.",
                    "WiiNote: multimodal application facilitating multi-user photo annotation activity.",
                    "Are gesture-based interfaces the future of human computer interaction?",
                    "Providing expressive eye movement to virtual agents.",
                    "Mediated attention with multimodal augmented reality.",
                    "Grounding spatial prepositions for video search.",
                    "Multi-modal and multi-camera attention in smart environments.",
                    "RVDT: a design space for multiple input devices, multipleviews and multiple display surfaces combination.",
                    "Learning and predicting multimodal daily life patterns from cell phones.",
                    "Visual based picking supported by context awareness: comparing picking performance using paper-based lists versus lists presented on a head mounted display with contextual support.",
                    "Adaptation from partially supervised handwritten text transcriptions.",
                    "Recognizing events with temporal random forests.",
                    "Activity-aware ECG-based patient authentication for remote health monitoring.",
                    "GaZIR: gaze-based zooming interface for image retrieval.",
                    "Voice key board: multimodal indic text input.",
                    "Evaluating the effect of temporal parameters for vibrotactile saltatory patterns.",
                    "Mapping information to audio and tactile icons.",
                    "Augmented reality target finding based on tactile cues.",
                    "Speaker change detection with privacy-preserving audio cues.",
                    "MirrorTrack: tracking with reflection - comparison with top-down approach.",
                    "A framework for continuous multimodal sign language recognition."
                ]
            }
        ]
    },
    {
        "year": "2008",
        "name": "10th ICMI 2008",
        "info": "Chania, Crete, Greece",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2008",
                "sub_name": "Proceedings of the 10th International Conference on Multimodal Interfaces, ICMI 2008, Chania, Crete, Greece, October 20-22, 2008.",
                "count": 53,
                "papers": [
                    "Natural interfaces in the field: the case of pen and paper.",
                    "Manipulating trigonometric expressions encodedthrough electro-tactile signals.",
                    "Multimodal system evaluation using modality efficiency and synergy metrics.",
                    "Effectiveness and usability of an online help agent embodied as a talking head.",
                    "Interaction techniques for the analysis of complex data on high-resolution displays.",
                    "Role recognition in multiparty recordings using social affiliation networks and discrete distributions.",
                    "Audiovisual laughter detection based on temporal features.",
                    "Predicting two facets of social verticality in meetings from five-minute time slices and nonverbal cues.",
                    "Multimodal recognition of personality traits in social interactions.",
                    "Social signals, their function, and automatic analysis: a survey.",
                    "VoiceLabel: using speech to label mobile sensor data.",
                    "The babbleTunes system: talk to your ipod!",
                    "Evaluating talking heads for smart home systems.",
                    "Perception of dynamic audiotactile feedback to gesture input.",
                    "An integrative recognition method for speech and gestures.",
                    "As go the feet...: on the estimation of attentional focus from stance.",
                    "Knowledge and data flow architecture for reference processing in multimodal dialog systems.",
                    "The CAVA corpus: synchronised stereoscopic and binaural datasets with head movements.",
                    "Towards a minimalist multimodal dialogue framework using recursive MVC pattern.",
                    "Explorative studies on multimodal interaction in a PDA- and desktop-based scenario.",
                    "Designing context-aware multimodal virtual environments.",
                    "A high-performance dual-wizard infrastructure for designing speech, pen, and multimodal interfaces.",
                    "The WAMI toolkit for developing, deploying, and evaluating web-accessible multimodal interfaces.",
                    "A three-dimensional characterization space of software components for rapidly developing multimodal interfaces.",
                    "Crossmodal congruence: the look, feel and sound of touchscreen widgets.",
                    "MultiML: a general purpose representation language for multimodal human utterances.",
                    "Deducing the visual focus of attention from head pose estimation in dynamic multi-view meeting scenarios.",
                    "Context-based recognition during human interactions: automatic feature selection and encoding dictionary.",
                    "AcceleSpell, a gestural interactive game to learn and practice finger spelling.",
                    "A multi-modal spoken dialog system for interactive TV.",
                    "Multimodal slideshow: demonstration of the openinterface interaction development environment.",
                    "A browser-based multimodal interaction system.",
                    "IGlasses: an automatic wearable speech supplementin face-to-face communication and classroom situations.",
                    "Innovative interfaces in MonAMI: the reminder.",
                    "PHANTOM prototype: exploring the potential for learning with multimodal features in dentistry.",
                    "Audiovisual 3d rendering as a tool for multimodal interfaces.",
                    "Multimodal presentation and browsing of music.",
                    "An audio-haptic interface based on auditory depth cues.",
                    "Detection and localization of 3d audio-visual objects using unsupervised clustering.",
                    "Robust gesture processing for multimodal interaction.",
                    "Investigating automatic dominance estimation in groups from visual attention and speaking activity.",
                    "Dynamic modality weighting for multi-stream hmms inaudio-visual speech recognition.",
                    "A Fitts Law comparison of eye tracking and manual input in the selection of visual targets.",
                    "A Wizard of Oz study for an AR multimodal interface.",
                    "A realtime multimodal system for analyzing group meetings by combining face pose tracking and speaker diarization.",
                    "Designing and evaluating multimodal interaction for mobile contexts.",
                    "Automated sip detection in naturally-evoked video.",
                    "Perception of low-amplitude haptic stimuli when biking.",
                    "TactiMote: a tactile remote control for navigating in long lists.",
                    "The DIRAC AWEAR audio-visual platform for detection of unexpected and incongruent events.",
                    "Smoothing human-robot speech interactions by using a blinking-light as subtle expression.",
                    "Feel-good touch: finding the most pleasant tactile feedback for a mobile touch screen button.",
                    "Embodied conversational agents for voice-biometric interfaces."
                ]
            }
        ]
    },
    {
        "year": "2007",
        "name": "9th ICMI 2007",
        "info": "Nagoya, Aichi, Japan",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2007",
                "sub_name": "Proceedings of the 9th International Conference on Multimodal Interfaces, ICMI 2007, Nagoya, Aichi, Japan, November 12-15, 2007.",
                "count": 60,
                "papers": [
                    "The painful face: pain expression recognition using active appearance models.",
                    "Faces of pain: automated measurement of spontaneousallfacial expressions of genuine and posed pain.",
                    "Visual inference of human emotion and behaviour.",
                    "Audiovisual recognition of spontaneous interest within conversations.",
                    "How to distinguish posed from spontaneous smiles using geometric features.",
                    "Eliciting, capturing and tagging spontaneous facialaffect in autism spectrum disorder.",
                    "Statistical segmentation and recognition of fingertip trajectories for a gesture interface.",
                    "A tactile language for intuitive human-robot communication.",
                    "Simultaneous prediction of dialog acts and address types in three-party conversations.",
                    "Developing and analyzing intuitive modes for interactive object modeling.",
                    "Extraction of important interactions in medical interviewsusing nonverbal information.",
                    "Towards smart meeting: enabling technologies and a real-world application.",
                    "Multimodalcues for addressee-hood in triadic communication with a human information retrieval agent.",
                    "The effect of input mode on inactivity and interaction times of multimodal systems.",
                    "Positional mapping: keyboard mapping based on characters writing positions for mobile devices.",
                    "Five-key text input using rhythmic mappings.",
                    "Toward content-aware multimodal tagging of personal photo collections.",
                    "A survey of affect recognition methods: audio, visual and spontaneous expressions.",
                    "Real-time expression cloning using appearance models.",
                    "Gaze-communicative behavior of stuffed-toy robot with joint attention and eye contact based on ambient gaze-tracking.",
                    "Map navigation with mobile devices: virtual versus physical movement with and without visual context.",
                    "Can you talk or only touch-talk: A VoIP-based phone feature for quick, quiet, and private communication.",
                    "Designing audio and tactile crossmodal icons for mobile devices.",
                    "A study on the scalability of non-preferred hand mode manipulation.",
                    "Voicepen: augmenting pen input with simultaneous non-linguisitic vocalization.",
                    "A large-scale behavior corpus including multi-angle video data for observing infants' long-term developmental processes.",
                    "The micole architecture: multimodal support for inclusion of visually impaired children.",
                    "Interfaces for musical activities and interfaces for musicians are not the same: the case for codes, a web-based environment for cooperative music prototyping.",
                    "Totalrecall: visualization and semi-automatic annotation of very large audio-visual corpora.",
                    "Extensible middleware framework for multimodal interfaces in distributed environments.",
                    "Temporal filtering of visual speech for audio-visual speech recognition in acoustically and visually challenging environments.",
                    "Reciprocal attentive communication in remote meeting with a humanoid robot.",
                    "Password management using doodles.",
                    "A computational model for spatial expression resolution.",
                    "Disambiguating speech commands using physical context.",
                    "Automatic inference of cross-modal nonverbal interactions in multiparty conversations: \"who responds to whom, when, and how?\" from gaze, head gestures, and utterances.",
                    "Influencing social dynamics in meetings through a peripheral display.",
                    "Using the influence model to recognize functional roles in meetings.",
                    "User impressions of a stuffed doll robot's facing direction in animation systems.",
                    "Speech-driven embodied entrainment character system with hand motion input in mobile environment.",
                    "Natural multimodal dialogue systems: a configurable dialogue and presentation strategies component.",
                    "Modeling human interaction resources to support the design of wearable multimodal systems.",
                    "Speech-filtered bubble ray: improving target acquisition on display walls.",
                    "Using pen input features as indices of cognitive load.",
                    "Automated generation of non-verbal behavior for virtual embodied characters.",
                    "Detecting communication errors from visual cues during the system's conversational turn.",
                    "Multimodal interaction analysis in a smart house.",
                    "A multi-modal mobile device for learning japanese kanji characters through mnemonic stories.",
                    "3d augmented mirror: a multimodal interface for string instrument learning and teaching with gesture support.",
                    "Interest estimation based on dynamic bayesian networks for visual attentive presentation agents.",
                    "On-line multi-modal speaker diarization.",
                    "Presentation sensei: a presentation training system using speech and image processing.",
                    "The world of mushrooms: human-computer interaction prototype systems for ambient intelligence.",
                    "Evaluation of haptically augmented touchscreen gui elements under cognitive load.",
                    "Multimodal interfaces in semantic interaction.",
                    "Workshop on tagging, mining and retrieval of human related activity information.",
                    "Workshop on massive datasets.",
                    "Interfacing life: a year in the life of a research lab.",
                    "The great challenge of multimodal interfacestowards symbiosis of human and robots.",
                    "Just in time learning: implementing principles of multimodal processing and learning for education."
                ]
            }
        ]
    },
    {
        "year": "2006",
        "name": "8th ICMI 2006",
        "info": "Banff, Alberta, Canada",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2006",
                "sub_name": "Proceedings of the 8th International Conference on Multimodal Interfaces, ICMI 2006, Banff, Alberta, Canada, November 2-4, 2006.",
                "count": 60,
                "papers": [
                    "Weight, weight, don't tell me.",
                    "Movement and music: designing gestural interfaces for computer-based musical instruments.",
                    "Mixing virtual and actual.",
                    "Collaborative multimodal photo annotation over digital paper.",
                    "MyConnector: analysis of context cues to predict human availability for communication.",
                    "Human perception of intended addressee during computer-assisted meetings.",
                    "Automatic detection of group functional roles in face to face interactions.",
                    "Speaker localization for microphone array-based ASR: the effects of accuracy on overlapping speech.",
                    "Automatic speech recognition for webcasts: how good is good enough and what to do when it isn't.",
                    "Cross-modal coordination of expressive strength between voice and gesture for personified media.",
                    "VirtualHuman: dialogic and affective interaction with virtual characters.",
                    "From vocal to multimodal dialogue management.",
                    "Human-Robot dialogue for joint construction tasks.",
                    "roBlocks: a robotic construction kit for mathematics and science education.",
                    "GSI demo: multiuser gesture/speech interaction over digital tables by wrapping single user applications.",
                    "Co-Adaptation of audio-visual speech and gesture classifiers.",
                    "Towards the integration of shape-related information in 3-D gestures and speech.",
                    "Which one is better?: information navigation techniques for spatially aware handheld displays.",
                    "Comparing the effects of visual-auditory and visual-tactile feedback on user performance: a meta-analysis.",
                    "Multimodal estimation of user interruptibility for smart mobile telephones.",
                    "Short message dictation on Symbian series 60 mobile phones.",
                    "The NIST smart data flow system II multimodal data transport infrastructure.",
                    "A contextual multimodal integrator.",
                    "Collaborative multimodal photo annotation over digital paper.",
                    "CarDialer: multi-modal in-vehicle cellphone control application.",
                    "Gender and age estimation system robust to pose variations.",
                    "A fast and robust 3D head pose and gaze estimation system.",
                    "Audio-visual emotion recognition in adult attachment interview.",
                    "Modeling naturalistic affective states via facial and vocal expressions recognition.",
                    "A 'need to know' system for group classification.",
                    "Spontaneous vs. posed facial behavior: automatic analysis of brow actions.",
                    "Gaze-X: adaptive affective multimodal interface for single-user office scenarios.",
                    "Human computing, virtual humans and artificial imperfection.",
                    "Using maximum entropy (ME) model to incorporate gesture cues for SU detection.",
                    "Salience modeling based on non-verbal modalities for spoken language understanding.",
                    "EM detection of common origin of multi-modal cues.",
                    "Prototyping novel collaborative multimodal systems: simulation, data collection and analysis tools for the next decade.",
                    "Combining audio and video to predict helpers' focus of attention in multiparty remote collaboration on physical tasks.",
                    "The role of psychological ownership and ownership markers in collaborative working environment.",
                    "Foundations of human computing: facial expression and emotion.",
                    "Human computing and machine understanding of human behavior: a survey.",
                    "Computing human faces for human viewers: automated animation in photographs and paintings.",
                    "Detection and application of influence rankings in small group meetings.",
                    "Tracking the multi person wandering visual focus of attention.",
                    "Toward open-microphone engagement for multiparty interactions.",
                    "Tracking head pose and focus of attention with multiple far-field cameras.",
                    "Recognizing gaze aversion gestures in embodied conversational discourse.",
                    "Explorations in sound for tilting-based interfaces.",
                    "Haptic phonemes: basic building blocks of haptic communication.",
                    "Toward haptic rendering for a virtual dissection.",
                    "Embrace system for remote counseling.",
                    "Enabling multimodal communications for enhancing the ability of learning for the visually impaired.",
                    "The benefits of multimodal information: a meta-analysis comparing visual and visual-tactile feedback.",
                    "Word graph based speech rcognition error correction by handwriting input.",
                    "Using redundant speech and handwriting for learning new vocabulary and understanding abbreviations.",
                    "Multimodal fusion: a new hybrid strategy for dialogue systems.",
                    "Evaluating usability based on multimodal information: an empirical study.",
                    "A new approach to haptic augmentation of the GUI.",
                    "HMM-based synthesis of emotional facial expressions during speech in synthetic talking heads.",
                    "Embodiment and multimodality."
                ]
            },
            {
                "sub_name_abbr": "conf/aihc/2007",
                "sub_name": "Artifical Intelligence for Human Computing, ICMI 2006 and IJCAI 2007 International Workshops, Banff, Canada, November 3, 2006, Hyderabad, India, January 6, 2007, Revised Seleced and Invited Papers.",
                "count": 17,
                "papers": [
                    "Foundations of Human Computing: Facial Expression and Emotion.",
                    "Instinctive Computing.",
                    "Human Computing and Machine Understanding of Human Behavior: A Survey.",
                    "Audio-Visual Spontaneous Emotion Recognition.",
                    "Modeling Naturalistic Affective States Via Facial, Vocal, and Bodily Expressions Recognition.",
                    "Emotion and Reinforcement: Affective Facial Expressions Facilitate Robot Learning.",
                    "Trajectory-Based Representation of Human Actions.",
                    "Modelling the Communication Atmosphere: A Human Centered Multimedia Approach to Evaluate Communicative Situations.",
                    "Modeling Influence Between Experts.",
                    "Social Intelligence Design and Human Computing.",
                    "Feedback Loops in Communication and Human Computing.",
                    "Evaluating the Future of HCI: Challenges for the Evaluation of Emerging Applications.",
                    "Gaze-X: Adaptive, Affective, Multimodal Interface for Single-User Office Scenarios.",
                    "SmartWeb Handheld - Multimodal Interaction with Ontological Knowledge Bases and Semantic Web Services.",
                    "A Learning-Based High-Level Human Computer Interface for Face Modeling and Animation.",
                    "Challenges for Virtual Humans in Human Computing.",
                    "Affect Detection and an Automated Improvisational AI Actor in E-Drama."
                ]
            }
        ]
    },
    {
        "year": "2005",
        "name": "7th ICMI 2005",
        "info": "Trento, Italy",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2005",
                "sub_name": "Proceedings of the 7th International Conference on Multimodal Interfaces, ICMI 2005, Trento, Italy, October 4-6, 2005.",
                "count": 47,
                "papers": [
                    "The \"puzzle\" of sensory perception: putting together multisensory information.",
                    "Integrating sketch and speech inputs using spatial information.",
                    "Distributed pointing for multimodal collaboration over sketched diagrams.",
                    "Contextual recognition of head gestures.",
                    "Combining environmental cues & head gestures to interact with wearable devices.",
                    "Automatic detection of interaction groups.",
                    "Meeting room configuration and multiple camera calibration in meeting analysis.",
                    "A multimodal perceptual user interface for video-surveillance environments.",
                    "Inferring body pose using speech content.",
                    "A joint particle filter for audio-visual speaker tracking.",
                    "The connector: facilitating context-aware communication.",
                    "A user interface framework for multimodal VR interactions.",
                    "Multimodal output specification / simulation platform.",
                    "Migratory MultiModal interfaces in MultiDevice environments.",
                    "Exploring multimodality in the laboratory and the field.",
                    "Understanding the effect of life-like interface agents through users' eye movements.",
                    "Analyzing and predicting focus of attention in remote collaborative tasks.",
                    "Gaze-based selection of standard-size menu items.",
                    "Region extraction of a gaze object using the gaze point and view image sequences.",
                    "Interactive humanoids and androids as ideal interfaces for humans.",
                    "Probabilistic grounding of situated speech using plan recognition and reference resolution.",
                    "Augmenting conversational dialogue by means of latent semantic googling.",
                    "Human-style interaction with a robot for cooperative learning of scene objects.",
                    "A look under the hood: design and development of the first SmartWeb system demonstrator.",
                    "Audio-visual cues distinguishing self- from system-directed speech in younger and older adults.",
                    "Identifying the intended addressee in mixed human-human and human-computer interaction from non-verbal features.",
                    "Multimodal multispeaker probabilistic tracking in meetings.",
                    "A probabilistic inference of multiparty-conversation structure based on Markov-switching models of gaze patterns, head directions, and utterances.",
                    "Socially aware computation and communication.",
                    "Synthetic characters as multichannel interfaces.",
                    "XfaceEd: authoring tool for embodied conversational agents.",
                    "A first evaluation study of a database of kinetic facial expressions (DaFEx).",
                    "Hapticat: exploration of affective touch.",
                    "Using observations of real designers at work to inform the development of a novel haptic modeling system.",
                    "A comparison of two methods of scaling on form perception via a haptic interface.",
                    "An initial usability assessment for symbolic haptic rendering of music parameters.",
                    "Tangible user interfaces for 3D clipping plane interaction with volumetric data: a case study.",
                    "A transformational approach for multimodal web user interfaces based on UsiXML.",
                    "A pattern mining method for interpretation of interaction.",
                    "A study of manual gesture-based selection for the PEMMI multimodal transport management interface.",
                    "Recognition of sign language subwords based on boosted hidden Markov models.",
                    "Gesture-driven American sign language phraselator.",
                    "Interactive vision to detect target objects for helper robots.",
                    "The contrastive evaluation of unimodal and multimodal interfaces for voice otput communication aids.",
                    "Agent-based architecture for implementing multimodal learning environments for visually impaired children.",
                    "Perceiving ordinal data haptically under workload.",
                    "Virtual tangible widgets: seamless universal interaction with personal sensing devices."
                ]
            }
        ]
    },
    {
        "year": "2004",
        "name": "6th ICMI 2004",
        "info": "State College, PA, USA",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2004",
                "sub_name": "Proceedings of the 6th International Conference on Multimodal Interfaces, ICMI 2004, State College, PA, USA, October 13-15, 2004.",
                "count": 69,
                "papers": [
                    "Two-way eye contact between humans and robots.",
                    "Another person's eye gaze as a cue in solving programming problems.",
                    "EyePrint: support of document browsing with eye gaze trace.",
                    "A framework for evaluating multimodal integration by humans and a role for embodied conversational agents.",
                    "From conversational tooltips to grounded discourse: head poseTracking in interactive dialog systems.",
                    "Evaluation of spoken multimodal conversation.",
                    "Multimodal transformed social interaction.",
                    "Multimodal interaction in an augmented reality scenario.",
                    "The ThreadMill architecture for stream-oriented human communication analysis applications.",
                    "TouchLight: an imaging touch screen and display for gesture-based interaction.",
                    "Walking-pad: a step-in-place locomotion interface for virtual environments.",
                    "Multimodal detection of human interaction events in a nursing home environment.",
                    "Elvis: situated speech and gesture understanding for a robotic chandelier.",
                    "Towards integrated microplanning of language and iconic gesture for multimodal output.",
                    "Exploiting prosodic structuring of coverbal gesticulation.",
                    "Visual and linguistic information in gesture classification.",
                    "Multimodal model integration for sentence unit detection.",
                    "When do we interact multimodally?: cognitive load and multimodal communication patterns.",
                    "Bimodal HCI-related affect recognition.",
                    "Identifying the addressee in human-human-robot interactions based on head pose and speech.",
                    "Articulatory features for robust visual speech recognition.",
                    "M/ORIS: a medical/operating room interaction system.",
                    "Modality fusion for graphic design applications.",
                    "Implementation and evaluation of a constraint-based multimodal fusion system for speech and 3D pointing gestures.",
                    "AROMA: ambient awareness through olfaction in a messaging application.",
                    "The virtual haptic back for palpatory training.",
                    "A vision-based sign language recognition system using tied-mixture density HMM.",
                    "Analysis of emotion recognition using facial expressions, speech and multimodal information.",
                    "Support for input adaptability in the ICON toolkit.",
                    "User walkthrough of multimodal access to multidimensional databases.",
                    "Multimodal interaction under exerted conditions in a natural field setting.",
                    "A segment-based audio-visual speech recognizer: data collection, development, and initial experiments.",
                    "A model-based approach for real-time embedded multimodal systems in military aircrafts.",
                    "ICARE software components for rapidly developing multimodal interfaces.",
                    "MacVisSTA: a system for multimodal analysis.",
                    "Context based multimodal fusion.",
                    "Emotional Chinese talking head system.",
                    "Experiences on haptic interfaces for visually impaired young children.",
                    "Visual touchpad: a two-handed gestural input device.",
                    "An evaluation of virtual human technology in informational kiosks.",
                    "Software infrastructure for multi-modal virtual environments.",
                    "GroupMedia: distributed multi-modal interfaces.",
                    "Agent and library augmented shared knowledge areas (ALASKA).",
                    "MULTIFACE: multimodal content adaptations for heterogeneous devices.",
                    "Command and control resource performance predictor(C2RP2).",
                    "A multi-modal architecture for cellular phones.",
                    "'SlidingMap': introducing and evaluating a new modality for map interaction.",
                    "Multimodal interaction for distributed collaboration.",
                    "A multimodal learning interface for sketch, speak and point creation of a schedule chart.",
                    "Real-time audio-visual tracking for meeting analysis.",
                    "Collaboration in parallel worlds.",
                    "Segmentation and classification of meetings using multiple information streams.",
                    "A maximum entropy based approach for multimodal integration.",
                    "Multimodal interface platform for geographical information systems (GeoMIP) in crisis management.",
                    "Adaptations of multimodal content in dialog systems targeting heterogeneous devices.",
                    "Utilizing gestures to better understand dynamic structure of human communication.",
                    "Multimodal programming for dyslexic students.",
                    "Gestural cues for speech understanding.",
                    "Using language structure for adaptive multimodal language acquisition.",
                    "Private speech during multimodal human-computer interaction.",
                    "Projection augmented models: the effect of haptic feedback on subjective and objective human factors.",
                    "Multimodal interface design for multimodal meeting content retrieval.",
                    "Determining efficient multimodal information-interaction spaces for C2 systems.",
                    "Using spatial warning signals to capture a driver's visual attention.",
                    "Multimodal interfaces and applications for visually impaired children.",
                    "Multilayer architecture in sign language recognition system.",
                    "Computer vision techniques and applications in human-computer interaction.",
                    "Multimodal response generation in GIS.",
                    "Adaptive multimodal recognition of voluntary and involuntary gestures of people with motor disabilities."
                ]
            }
        ]
    },
    {
        "year": "2003",
        "name": "5th ICMI 2003",
        "info": "Vancouver, British Columbia, Canada",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2003",
                "sub_name": "Proceedings of the 5th International Conference on Multimodal Interfaces, ICMI 2003, Vancouver, British Columbia, Canada, November 5-7, 2003.",
                "count": 50,
                "papers": [
                    "Multimodal user interfaces: who's the user?",
                    "New techniques for evaluating innovative interfaces with eye tracking.",
                    "Crossmodal attention and multisensory integration: implications for multimodal interface design.",
                    "A system for fast, full-text entry for small electronic devices.",
                    "Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality.",
                    "Learning and reasoning about interruption.",
                    "Providing the basis for human-robot-interaction: a multi-modal attention system for a mobile robot.",
                    "Selective perception policies for guiding sensing and computation in multimodal systems: a comparative analysis.",
                    "Toward a theory of organized multimodal integration patterns during human-computer interaction.",
                    "TorqueBAR: an ungrounded haptic feedback device.",
                    "Towards tangibility in gameplay: building a tangible affective interface for a computer game.",
                    "Multimodal biometrics: issues in design and testing.",
                    "Sensitivity to haptic-audio asynchrony.",
                    "A multi-modal approach for determining speaker location and focus.",
                    "Distributed and local sensing techniques for face-to-face collaboration.",
                    "Georgia tech gesture toolkit: supporting experiments in gesture recognition.",
                    "Architecture and implementation of multimodal plug and play.",
                    "SmartKom: adaptive and flexible multimodal access to multiple applications.",
                    "A framework for rapid development of multimodal interfaces.",
                    "Capturing user tests in a multimodal, multidevice informal prototyping tool.",
                    "Large vocabulary sign language recognition based on hierarchical decision trees.",
                    "Hand motion gestural oscillations and multimodal discourse.",
                    "Pointing gesture recognition based on 3D-tracking of face, hands and head orientation.",
                    "Untethered gesture acquisition and recognition for a multimodal conversational system.",
                    "Where is \"it\"? Event Synchronization in Gaze-Speech Input Systems.",
                    "Eyetracking in cognitive state detection for HCI.",
                    "A multimodal learning interface for grounding spoken language in sensory perceptions.",
                    "A computer-animated tutor for spoken and written language learning.",
                    "Augmenting user interfaces with adaptive speech commands.",
                    "Combining speech and haptics for intuitive and efficient navigation through image databases.",
                    "Interactive skills using active gaze tracking.",
                    "Error recovery in a blended style eye gaze and speech interface.",
                    "Using an autonomous cube for basic navigation and input.",
                    "GWindows: robust stereo vision for gesture-based control of windows.",
                    "A visually grounded natural language interface for reference to spatial scenes.",
                    "Perceptual user interfaces using vision-based eye tracking.",
                    "Sketching informal presentations.",
                    "Gestural communication over video stream: supporting multimodal interaction for remote collaborative physical tasks.",
                    "The role of spoken feedback in experiencing multimodal interfaces as human-like.",
                    "Real time facial expression recognition in video using support vector machines.",
                    "Modeling multimodal integration patterns and performance in seniors: toward adaptive processing of individual differences.",
                    "Auditory, graphical and haptic contact cues for a reach, grasp, and place task in an augmented environment.",
                    "Mouthbrush: drawing and painting by hand and mouth.",
                    "XISL: a language for describing multimodal interaction scenarios.",
                    "IRYS: a visualization tool for temporal analysis of multimodal interaction.",
                    "Towards robust person recognition on handheld devices using face and speaker identification technologies.",
                    "Algorithms for controlling cooperation between output modalities in 2D embodied conversational agents.",
                    "Towards an attentive robotic dialog partner.",
                    "Demo: a multi-modal training environment for surgeons.",
                    "Demo: playingfFantasyA with senToy."
                ]
            }
        ]
    },
    {
        "year": "2002",
        "name": "4th ICMI 2002",
        "info": "Pittsburgh, Pennsylvania, USA",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2002",
                "sub_name": "4th IEEE International Conference on Multimodal Interfaces (ICMI 2002), 14-16 October 2002, Pittsburgh, PA, USA.",
                "count": 87,
                "papers": [
                    "Layered Representations for Human Activity Recognition.",
                    "Evaluating Integrated Speech- and Image Understanding.",
                    "Techniques for Interactive Audience Participation.",
                    "Perceptual Collaboration in Neem.",
                    "A Tracking Framework for Collaborative Human Computer Interaction.",
                    "A Structural Approach to Distance Rendering in Personal Auditory Displays.",
                    "A Multimodal Electronic Travel Aid Device.",
                    "Lecture and Presentation Tracking in an Intelligent Meeting Room.",
                    "Parallel Computing-Based Architecture for Mixed-Initiative Spoken Dialogue.",
                    "3-D N-Best Search for Simultaneous Recognition of Distant-Talking Speech of Multiple Talkers.",
                    "Integration of Tone Related Feature for Chinese Speech Recognition.",
                    "Talking Heads: Which Matching between Faces and Synthetic Voices?.",
                    "Robust Noisy Speech Recognition with Adaptive Frequency Bank Selection.",
                    "Covariance-Tied Clustering Method In Speaker Identification.",
                    "Context-Based Multimodal Input Understanding in Conversational Systems.",
                    "Context-Sensitive Help for Multimodal Dialogue.",
                    "Referring to Objects with Spoken and Haptic Modalities.",
                    "Towards Visually-Grounded Spoken Language Acquisition.",
                    "Modeling Output in the EMBASSI Multimodal Dialog System.",
                    "Multimodal Dialogue Systems for Interactive TVApplications.",
                    "Human - Robot Interaction: Engagement between Humans and Robots for Hosting Activities.",
                    "Viewing and Analyzing Multimodal Human-computer Tutorial Dialogue: A Database Approach.",
                    "Adaptive Dialog Based upon Multimodal Language Acquisition.",
                    "Integrating Emotional Cues into a Framework for Dialogue Management.",
                    "Data Driven Design of an ANN/HMM System for On-line Unconstrained Handwritten Character Recognition.",
                    "Gesture Patterns during Speech Repairs.",
                    "Prosody Based Co-analysis for Continuous Recognition of Coverbal Gestures.",
                    "Purdue RVL-SLLL ASL Database for Automatic Recognition of American Sign Language.",
                    "The Role of Gesture in Multimodal Referring Actions.",
                    "Hand Gesture Symmetric Behavior Detection and Analysis in Natural Conversation.",
                    "A Multi-Class Pattern Recognition System for Practical Finger Spelling Translation.",
                    "A Map-Based System Using Speech and 3D Gestures for Pervasive Computing.",
                    "Hand Tracking Using Spatial Gesture Modeling and Visual Feedback for a Virtual DJ System.",
                    "State Sharing in a Hybrid Neuro-Markovian On-Line Handwriting Recognition System through a Simple Hierarchical Clustering Algorithm.",
                    "An Automatic Speech Translation System on PDAs for Travel Conversation.",
                    "A PDA-Based Sign Translator.",
                    "The NESPOLE! Multimodal Interface for Cross-lingual Communication - Experience and Lessons Learned .",
                    "Research of Machine Learning Method for Specific Information Recognition on the Internet.",
                    "The Added Value of Multimodality in the NESPOLE! Speech-to-Speech Translation System: an Experimental Study.",
                    "Multi-Modal Translation System and Its Evaluation.",
                    "Towards Universal Speech Recognition.",
                    "Improved Named Entity Translation and Bilingual Named Entity Extraction.",
                    "Active Gaze Tracking for Human-Robot Interaction.",
                    "3-D Articulated Pose Tracking for Untethered Diectic Reference.",
                    "Tracking Focus of Attention in Meetings.",
                    "A Probabilistic Dynamic Contour Model for Accurate and Robust Lip Tracking.",
                    "Attentional Object Spotting by Integrating Multimodal Input.",
                    "Lip Tracking for MPEG-4 Facial Animation.",
                    "Achieving Real-Time Lip Synch via SVM-Based Phoneme Classification and Lip Shape Refinement.",
                    "Multi-Modal Temporal Asynchronicity Modeling by Product HMMs for Robust.",
                    "A Multi-Modal Interface for an Interactive Simulated Vascular Reconstruction System.",
                    "Universal Interfaces to Multimedia Documents.",
                    "A Video Based Interface to Textual Information for the Visually Impaired.",
                    "Modular Approach of Multimodal Integration in a Virtual Environment.",
                    "Mobile Multi-Modal Data Services for GPRS Phones and Beyond.",
                    "Flexi-Modal and Multi-Machine User Interfaces.",
                    "A Real-Time Framework for Natural Multimodal Interaction with Large Screen Displays.",
                    "Embarking on Multimodal Interface Design.",
                    "Multi Modal User Interaction in an Automatic Pool Trainer.",
                    "Multimodal Contextual Car-Driver Interface.",
                    "Requirements for Automatically Generating Multi-Modal Interfaces for Complex Appliances.",
                    "Articulated Model Based People Tracking Using Motion Models.",
                    "Audiovisual Arrays for Untethered Spoken Interfaces.",
                    "Fingerprint Classification by Directional Fields.",
                    "Towards Vision-Based 3-D People Tracking in a Smart Room.",
                    "Using TouchPad Pressure to Detect Negative Affect.",
                    "Designing Transition Networks for Multimodal VR-Interactions Using a Markup Language.",
                    "Musically Expressive Doll in Face-to-Face Communication.",
                    "Towards Monitoring Human Activities Using an Omnidirectional Camera.",
                    "Smart Platform - A Software Infrastructure for Smart Space (SISS).",
                    "Do Multimodal Signals Need to Come from the Same Place? Crossmodal Attentional Links Between Proximal and Distal Surfaces.",
                    "CATCH-2004 Multi-Modal Browser: Overview Description with Usability Analysis.",
                    "Multimodal Interaction During Multiparty Dialogues: Initial Results.",
                    "Multi-Modal Embodied Agents Scripting.",
                    "A Methodology for Evaluating Multimodality in a Home Entertainment System.",
                    "Body-Based Interfaces.",
                    "Evaluation of the Command and Control Cube.",
                    "Interruptions as Multimodal Outputs: Which are the Less Disruptive?.",
                    "Experimentally Augmenting an Intelligent Tutoring System with Human-Supplied Capabilities: Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens.",
                    "Individual Differences in Facial Expression: Stability over Time, Relation to Self-Reported Emotion, and Ability to Inform Person Identification.",
                    "Training a Talking Head.",
                    "Labial Coarticulation Modeling for Realistic Facial Animation.",
                    "Improved Information Maximization based Face and Facial Feature Detection from Real-time Video and Application in a Multi-Modal Person Identification System.",
                    "Animating Arbitrary Topology 3D Facial Model Using the MPEG-4 FaceDefTables.",
                    "An Improved Active Shape Model for Face Alignment.",
                    "Head-Pose Invariant Facial Expression Recognition Using Convolutional Neural Networks.",
                    "An Improved Algorithm for Hairstyle Dynamics."
                ]
            }
        ]
    },
    {
        "year": "2000",
        "name": "3rd ICMI 2000",
        "info": "Beijing, China",
        "venues": [
            {
                "sub_name_abbr": "conf/icmi/2000",
                "sub_name": "Advances in Multimodal Interfaces - ICMI 2000, Third International Conference, Beijing, China, October 14-16, 2000, Proceedings.",
                "count": 86,
                "papers": [
                    "Gaze and Speech in Attentive User Interfaces.",
                    "Resolving References to Graphical Objects in Multimodal Queries by Constraint Satisfaction.",
                    "Human-Robot Interface Based on Speech Understanding Assisted by Vision.",
                    "Designing Multi-sensory Models for Finding Patterns in Stock Market Data.",
                    "Ausio-visual Segmentation and \"The Cocktail Party Effect\".",
                    "Visual Recognition of Emotional States.",
                    "An Experimental Study of Input Modes for Multimodal Human-Computer Interaction.",
                    "Emotion Expression Functions in Multimodal Presentation.",
                    "A Sound MagicBoard.",
                    "A Head Gesture Recognition Algorithm.",
                    "Gestures Recognition for a Walking Person by Using a Sequence of Spatially Reduced Range Image.",
                    "Virtual Mouse - Inputting Device by Hand Gesture Tracking and Recognition.",
                    "A Self-reliance Smoothness of the Vision-Based Recognition of Hand Gestures.",
                    "Hand Shape Extraction and Understanding by Virtue of Multiple Cues Fusion Technology.",
                    "Head Pose Estimation for Video-Conferencing with Multiple Cameras and Microphones.",
                    "A General Framework for Face Detection.",
                    "Glasses Detection for Face Recognition Using Bayes Rules.",
                    "Real-Time Face Tracking under Partial Occlusion and Illumination Change.",
                    "Eye-State Action Unit Detection by Gabor Wavelets.",
                    "Web-PICASSO: Internet Implementation of Facial Caricature System PICASSO.",
                    "Recognition of Human Faces Based on Fast Computation of Circular Harmonic Components.",
                    "Multiple Faces Tracking and Zooming System.",
                    "Face Warping Using a Single Image.",
                    "Hierarchical Framework for Facial Expression Recognition.",
                    "Detecting Facial Features on Images with Multiple Faces.",
                    "The Synthesis of Realistic Human Face.",
                    "Gravity-Center Template Based Human Face Feature Detection.",
                    "An Improved Facial Expression Recognition Method.",
                    "Face Direction Estimation Using Multiple Cameras for Human Computer Interaction.",
                    "Face Recognition Based on Local Fisher Features.",
                    "Combining Skin Color Model and Neural Network for Rotation Invariant Face Detection.",
                    "Multimodal Integration Using Complex Feature Set.",
                    "Bilingual Dictionary Based Sentence Alignment for Chinese English Bitext.",
                    "A Task Oriented Natural Language Understanding Model.",
                    "Variable Length Language Model for Chinese Character Recognition.",
                    "Automatic Lexical Errors Detecting of Chinese Texts Based on the Orderly-Neighborship.",
                    "Statistical Analysis of Chinese Language and Language Modeling Based on Huge Text Corpora.",
                    "On Building a Simulating Translation Environment for Multilingual Conversation.",
                    "Approach to Recognition and Understanding of the Time Constituents in the Spoken Chinese Language Translation.",
                    "KD2000 Chinese Text-To-Speech System.",
                    "Multimodal Speaker Detection Using Input/Output Dynamic Bayesian Networks.",
                    "Research on Speech Recognition Based on Phase Space Reconstruction Theory.",
                    "Estimating the Pose of Phicons for Human Computer Interaction.",
                    "An Approach to Robust and Fast Locating Lip Motion.",
                    "Multi-level Human Tracking.",
                    "Region-Based Tracking in Video Sequences Using Planar Perspective Models.",
                    "A Novel Motion Estimation Algorithm Based on Dynamic Search Window and Spiral Search.",
                    "Determining Motion Components Using the Point Distribution Model.",
                    "Automatic Recognition of Unconstrained Off-Line Bangla Handwritten Numerals.",
                    "A Novel Algorithm for Handwritten Chinese Character Recognition.",
                    "An HMM Based Two-Pass Approach for Off-Line Cursive Handwriting Recognition.",
                    "On-Line Recognition of Mathematical Expressions Using Automatic Rewriting Method.",
                    "On-Line Hand-Drawn Symbol Recognition Based on Primitives Separation and Fuzzy Inference.",
                    "Integration MBHMM and Neural Network for Totally Unconstrained Handwritten Numerals Recognition.",
                    "Aspect Ratio Adaptive Normalization for Handwritten Character Recognition.",
                    "A Neural-Network Dimension Reduction Method for Large-Set Pattern Classification.",
                    "Local Subspace Classifier in Reproducing Kernel Hilbert Space.",
                    "A Recognition System for Devnagri and English Handwritten Numerals.",
                    "Deformation Transformation for Handwritten Chinese Character Shape Correction.",
                    "Offline Handwritten Chinese Character Recognition Using Optimal Sampling Features.",
                    "A New Multi-classifier Combination Scheme and Its Application in Handwriting Chinese Character Recognition.",
                    "Off-Line Handwritten Chinese Character Recognition with Nonlinear Pre-classification.",
                    "Detection of the Indicated Area with an Indication Stick.",
                    "Heuristic Walkthroughs Evaluation of Pen-Based Chinese Word Edit System (PCWES) Usability.",
                    "Design and Realization of 3D Space Coordinate Serial Input.",
                    "User's Vision Based Multi-resolution Rendering of 3D Models in Distributed Virtual Environment DVENET.",
                    "Vision-Based Registration Using 3-D Fiducial for Augmented Reality.",
                    "The Study on Stereoscopic Image Generating Algorithm Based on Image Transformation.",
                    "Jacob - An Animated Instruction Agent in Virtual Reality.",
                    "Penbuilder: Platform for the Development of Pen-Based User Interface.",
                    "Information Presentation for a Wearable Messenger Device.",
                    "Usability of Browser-Based Pen-Touch/Speech User Interfaces for Form-Based Application in Mobile Environment.",
                    "A Lock Opening and Closing System with the Image Base Using a Cellular Phone through the Internet.",
                    "HandTalker:  A Multimodal Dialog System Using Sign Language and 3-D Virtual Human.",
                    "A Vision-Based Method for Recognizing Non-manual Information in Japanese Sign Language.",
                    "A Parallel Multistream Model for Integration of Sign Language Recognition and Lip Motion.",
                    "Multi-modal Navigation for Interactive Wheelchair.",
                    "A Fast Sign Word Recognition Method for Chinese Sign Language.",
                    "AutomaticKeystone Correction for Camera-Assisted Presentation Interfaces.",
                    "A Multimodal User Interface for Geoscientific Data Investigation.",
                    "Partial Information in Multimodal Dialogue.",
                    "Multimodal Interface Techniques in Content-Based Multimedia Retrieval.",
                    "Design of Retrieval Method of Map Targets in the Multimodal Interface.",
                    "A First-Personness Approach to Co-operative Multimodal Interaction.",
                    "A Pragmatic Semantic Reliable Multicast Architecture for Distant Learning.",
                    "A Framework for Supporting Multimodal Conversational Characters in a Multi-agent System."
                ]
            }
        ]
    },
    {
        "year": "1999",
        "name": "2nd ICMI 1999",
        "info": "Hong Kong, China",
        "venues": []
    },
    {
        "year": "1996",
        "name": "1st ICMI 1996",
        "info": "Beijing, China",
        "venues": []
    }
]